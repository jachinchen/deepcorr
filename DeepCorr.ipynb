{"cells":[{"cell_type":"code","execution_count":45,"source":["def model(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","    flat_layers_after = [flow_size * 2, 1000, 50, 1]\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":46,"source":["def model_cnn(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","\n","    CNN_LAYERS = [[2, 20, 1, 2000, 5], [4, 10, 2000, 800, 3]]\n","\n","    for cnn_size in range(len(CNN_LAYERS)):\n","        cnn_weights = tf.get_variable(\n","            \"cnn_weight%d\" % cnn_size,\n","            CNN_LAYERS[cnn_size][:-1],\n","            initializer=tf.random_normal_initializer(stddev=0.01))\n","        cnn_bias = tf.get_variable(\"cnn_bias%d\" % cnn_size,\n","                                   [CNN_LAYERS[cnn_size][-2]],\n","                                   initializer=tf.zeros_initializer())\n","\n","        _x = tf.nn.conv2d(last_layer,\n","                          cnn_weights,\n","                          strides=[1, 2, 2, 1],\n","                          padding='VALID')\n","        _x = tf.nn.bias_add(_x, cnn_bias)\n","        conv = tf.nn.relu(_x, name='relu_cnn_%d' % cnn_size)\n","        pool = tf.nn.max_pool(conv,\n","                              ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1],\n","                              strides=[1, 1, 1, 1],\n","                              padding='VALID')\n","        last_layer = pool\n","    last_layer = tf.reshape(last_layer, [batch_size, -1])\n","\n","    flat_layers_after = [49600, 3000, 800, 100, 1]\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":48,"source":["def model_cnn(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","\n","    CNN_LAYERS = [[2, 20, 1, 2000, 5], [4, 10, 2000, 800, 3]]\n","\n","    for cnn_size in range(len(CNN_LAYERS)):\n","        cnn_weights = tf.get_variable(\n","            \"cnn_weight%d\" % cnn_size,\n","            CNN_LAYERS[cnn_size][:-1],\n","            initializer=tf.random_normal_initializer(stddev=0.01))\n","        cnn_bias = tf.get_variable(\"cnn_bias%d\" % cnn_size,\n","                                   [CNN_LAYERS[cnn_size][-2]],\n","                                   initializer=tf.zeros_initializer())\n","\n","        _x = tf.nn.conv2d(last_layer,\n","                          cnn_weights,\n","                          strides=[1, 2, 2, 1],\n","                          padding='VALID')\n","        _x = tf.nn.bias_add(_x, cnn_bias)\n","        conv = tf.nn.relu(_x, name='relu_cnn_%d' % cnn_size)\n","        pool = tf.nn.max_pool(conv,\n","                              ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1],\n","                              strides=[1, 1, 1, 1],\n","                              padding='VALID')\n","        last_layer = pool\n","    last_layer = tf.reshape(last_layer, [batch_size, -1])\n","\n","    flat_layers_after = [49600, 3000, 800, 100, 1]\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":49,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"error","ename":"TypeError","evalue":"Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=362'>363</a>\u001b[0m         \u001b[0;31m# Look up embeddings for inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=363'>364</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=364'>365</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=365'>366</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=366'>367</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36mmodel_cnn\u001b[0;34m(flow_before, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=274'>275</a>\u001b[0m                               padding='VALID')\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=275'>276</a>\u001b[0m         \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=276'>277</a>\u001b[0;31m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=277'>278</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=278'>279</a>\u001b[0m     \u001b[0mflat_layers_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m49600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6198\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6199\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   6200\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64"]}],"metadata":{}},{"cell_type":"code","execution_count":50,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(int(train_flow_before), int(dropout_keep_prob))\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"error","ename":"TypeError","evalue":"int() argument must be a string, a bytes-like object or a number, not 'Tensor'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=362'>363</a>\u001b[0m         \u001b[0;31m# Look up embeddings for inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=363'>364</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=364'>365</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=365'>366</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=366'>367</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'Tensor'"]}],"metadata":{}},{"cell_type":"code","execution_count":51,"source":["def model_cnn(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","\n","    CNN_LAYERS = [[2, 20, 1, 2000, 5], [4, 10, 2000, 800, 3]]\n","\n","    for cnn_size in range(len(CNN_LAYERS)):\n","        cnn_weights = tf.get_variable(\n","            \"cnn_weight%d\" % cnn_size,\n","            CNN_LAYERS[cnn_size][:-1],\n","            initializer=tf.random_normal_initializer(stddev=0.01))\n","        cnn_bias = tf.get_variable(\"cnn_bias%d\" % cnn_size,\n","                                   [CNN_LAYERS[cnn_size][-2]],\n","                                   initializer=tf.zeros_initializer())\n","\n","        _x = tf.nn.conv2d(last_layer,\n","                          cnn_weights,\n","                          strides=[1, 2, 2, 1],\n","                          padding='VALID')\n","        _x = tf.nn.bias_add(_x, cnn_bias)\n","        conv = tf.nn.relu(_x, name='relu_cnn_%d' % cnn_size)\n","        pool = tf.nn.max_pool(conv,\n","                              ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1],\n","                              strides=[1, 1, 1, 1],\n","                              padding='VALID')\n","        last_layer = pool\n","    last_layer = tf.reshape(int(last_layer), [batch_size, -1])\n","\n","    flat_layers_after = [49600, 3000, 800, 100, 1]\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":52,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before), dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-52-9898aa692b80>, line 66)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-52-9898aa692b80>\"\u001b[0;36m, line \u001b[0;32m66\u001b[0m\n\u001b[0;31m    y2 = model_cnn(train_flow_before), dropout_keep_prob)\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"metadata":{}},{"cell_type":"code","execution_count":53,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before), dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-53-9898aa692b80>, line 66)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-53-9898aa692b80>\"\u001b[0;36m, line \u001b[0;32m66\u001b[0m\n\u001b[0;31m    y2 = model_cnn(train_flow_before), dropout_keep_prob)\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"metadata":{}},{"cell_type":"code","execution_count":54,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.int,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.int,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.int,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before), dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-54-5b09c53af24c>, line 66)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-54-5b09c53af24c>\"\u001b[0;36m, line \u001b[0;32m66\u001b[0m\n\u001b[0;31m    y2 = model_cnn(train_flow_before), dropout_keep_prob)\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"metadata":{}},{"cell_type":"code","execution_count":55,"source":["def model_cnn(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","\n","    CNN_LAYERS = [[2, 20, 1, 2000, 5], [4, 10, 2000, 800, 3]]\n","\n","    for cnn_size in range(len(CNN_LAYERS)):\n","        cnn_weights = tf.get_variable(\n","            \"cnn_weight%d\" % cnn_size,\n","            CNN_LAYERS[cnn_size][:-1],\n","            initializer=tf.random_normal_initializer(stddev=0.01))\n","        cnn_bias = tf.get_variable(\"cnn_bias%d\" % cnn_size,\n","                                   [CNN_LAYERS[cnn_size][-2]],\n","                                   initializer=tf.zeros_initializer())\n","\n","        _x = tf.nn.conv2d(last_layer,\n","                          cnn_weights,\n","                          strides=[1, 2, 2, 1],\n","                          padding='VALID')\n","        _x = tf.nn.bias_add(_x, cnn_bias)\n","        conv = tf.nn.relu(_x, name='relu_cnn_%d' % cnn_size)\n","        pool = tf.nn.max_pool(conv,\n","                              ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1],\n","                              strides=[1, 1, 1, 1],\n","                              padding='VALID')\n","        last_layer = pool\n","    last_layer = tf.reshape(last_layer, [batch_size, -1])\n","\n","    flat_layers_after = [49600, 3000, 800, 100, 1]\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":56,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before), dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-56-9898aa692b80>, line 66)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-56-9898aa692b80>\"\u001b[0;36m, line \u001b[0;32m66\u001b[0m\n\u001b[0;31m    y2 = model_cnn(train_flow_before), dropout_keep_prob)\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"metadata":{}},{"cell_type":"code","execution_count":57,"source":["def generate_data(dataset, train_index, test_index, flow_size):\n","\n","    global negetive_samples\n","\n","    all_samples = len(train_index)\n","    labels = np.zeros((all_samples * (negetive_samples + 1), 1))\n","    l2s = np.zeros((all_samples * (negetive_samples + 1), 8, flow_size, 1))\n","\n","    index = 0\n","    random_ordering = [] + train_index\n","    for i in tqdm.tqdm(train_index):\n","        #[]#list(lsh.find_k_nearest_neighbors((Y_train[i]/ np.linalg.norm(Y_train[i])).astype(np.float64),(50)))\n","\n","        l2s[index, 0, :,\n","            0] = np.array(dataset[i]['here'][0]['<-'][:flow_size]) * 1000.0\n","        l2s[index, 1, :,\n","            0] = np.array(dataset[i]['there'][0]['->'][:flow_size]) * 1000.0\n","        l2s[index, 2, :,\n","            0] = np.array(dataset[i]['there'][0]['<-'][:flow_size]) * 1000.0\n","        l2s[index, 3, :,\n","            0] = np.array(dataset[i]['here'][0]['->'][:flow_size]) * 1000.0\n","\n","        l2s[index, 4, :,\n","            0] = np.array(dataset[i]['here'][1]['<-'][:flow_size]) / 1000.0\n","        l2s[index, 5, :,\n","            0] = np.array(dataset[i]['there'][1]['->'][:flow_size]) / 1000.0\n","        l2s[index, 6, :,\n","            0] = np.array(dataset[i]['there'][1]['<-'][:flow_size]) / 1000.0\n","        l2s[index, 7, :,\n","            0] = np.array(dataset[i]['here'][1]['->'][:flow_size]) / 1000.0\n","\n","        if index % (negetive_samples + 1) != 0:\n","            print(index, len(nears))\n","            raise\n","        labels[index, 0] = 1\n","        m = 0\n","        index += 1\n","        np.random.shuffle(random_ordering)\n","        for idx in random_ordering:\n","            if idx == i or m > (negetive_samples - 1):\n","                continue\n","\n","            m += 1\n","\n","            l2s[index, 0, :, 0] = np.array(\n","                dataset[idx]['here'][0]['<-'][:flow_size]) * 1000.0\n","            l2s[index, 1, :, 0] = np.array(\n","                dataset[i]['there'][0]['->'][:flow_size]) * 1000.0\n","            l2s[index, 2, :, 0] = np.array(\n","                dataset[i]['there'][0]['<-'][:flow_size]) * 1000.0\n","            l2s[index, 3, :, 0] = np.array(\n","                dataset[idx]['here'][0]['->'][:flow_size]) * 1000.0\n","\n","            l2s[index, 4, :, 0] = np.array(\n","                dataset[idx]['here'][1]['<-'][:flow_size]) / 1000.0\n","            l2s[index, 5, :, 0] = np.array(\n","                dataset[i]['there'][1]['->'][:flow_size]) / 1000.0\n","            l2s[index, 6, :, 0] = np.array(\n","                dataset[i]['there'][1]['<-'][:flow_size]) / 1000.0\n","            l2s[index, 7, :, 0] = np.array(\n","                dataset[idx]['here'][1]['->'][:flow_size]) / 1000.0\n","\n","            #l2s[index,0,:,0]=Y_train[i]#np.concatenate((Y_train[i],X_train[idx]))#(Y_train[i]*X_train[idx])/(np.linalg.norm(Y_train[i])*np.linalg.norm(X_train[idx]))\n","            #l2s[index,1,:,0]=X_train[idx]\n","\n","            labels[index, 0] = 0\n","            index += 1\n","\n","    #lsh.setup((X_test / np.linalg.norm(X_test,axis=1,keepdims=True)) .astype(np.float64))\n","    index_hard = 0\n","    num_hard_test = 0\n","    l2s_test = np.zeros(\n","        (len(test_index) * (negetive_samples + 1), 8, flow_size, 1))\n","    labels_test = np.zeros((len(test_index) * (negetive_samples + 1)))\n","    l2s_test_hard = np.zeros((num_hard_test * num_hard_test, 2, flow_size, 1))\n","    index = 0\n","    random_test = [] + test_index\n","\n","    for i in tqdm.tqdm(test_index):\n","        #list(lsh.find_k_nearest_neighbors((Y_test[i]/ np.linalg.norm(Y_test[i])).astype(np.float64),(50)))\n","\n","        if index % (negetive_samples + 1) != 0:\n","            print(index, nears)\n","            raise\n","        m = 0\n","\n","        np.random.shuffle(random_test)\n","        for idx in random_test:\n","            if idx == i or m > (negetive_samples - 1):\n","                continue\n","\n","            m += 1\n","            l2s_test[index, 0, :, 0] = np.array(\n","                dataset[idx]['here'][0]['<-'][:flow_size]) * 1000.0\n","            l2s_test[index, 1, :, 0] = np.array(\n","                dataset[i]['there'][0]['->'][:flow_size]) * 1000.0\n","            l2s_test[index, 2, :, 0] = np.array(\n","                dataset[i]['there'][0]['<-'][:flow_size]) * 1000.0\n","            l2s_test[index, 3, :, 0] = np.array(\n","                dataset[idx]['here'][0]['->'][:flow_size]) * 1000.0\n","\n","            l2s_test[index, 4, :, 0] = np.array(\n","                dataset[idx]['here'][1]['<-'][:flow_size]) / 1000.0\n","            l2s_test[index, 5, :, 0] = np.array(\n","                dataset[i]['there'][1]['->'][:flow_size]) / 1000.0\n","            l2s_test[index, 6, :, 0] = np.array(\n","                dataset[i]['there'][1]['<-'][:flow_size]) / 1000.0\n","            l2s_test[index, 7, :, 0] = np.array(\n","                dataset[idx]['here'][1]['->'][:flow_size]) / 1000.0\n","            labels_test[index] = 0\n","            index += 1\n","\n","        l2s_test[index, 0, :, 0] = np.array(\n","            dataset[i]['here'][0]['<-'][:flow_size]) * 1000.0\n","        l2s_test[index, 1, :, 0] = np.array(\n","            dataset[i]['there'][0]['->'][:flow_size]) * 1000.0\n","        l2s_test[index, 2, :, 0] = np.array(\n","            dataset[i]['there'][0]['<-'][:flow_size]) * 1000.0\n","        l2s_test[index, 3, :, 0] = np.array(\n","            dataset[i]['here'][0]['->'][:flow_size]) * 1000.0\n","\n","        l2s_test[index, 4, :, 0] = np.array(\n","            dataset[i]['here'][1]['<-'][:flow_size]) / 1000.0\n","        l2s_test[index, 5, :, 0] = np.array(\n","            dataset[i]['there'][1]['->'][:flow_size]) / 1000.0\n","        l2s_test[index, 6, :, 0] = np.array(\n","            dataset[i]['there'][1]['<-'][:flow_size]) / 1000.0\n","        l2s_test[index, 7, :, 0] = np.array(\n","            dataset[i]['here'][1]['->'][:flow_size]) / 1000.0\n","        #l2s_test[index,2,:,0]=dataset[i]['there'][0]['->'][:flow_size]\n","        #l2s_test[index,3,:,0]=dataset[i]['here'][0]['<-'][:flow_size]\n","\n","        #l2s_test[index,0,:,1]=dataset[i]['here'][1]['->'][:flow_size]\n","        #l2s_test[index,1,:,1]=dataset[i]['there'][1]['<-'][:flow_size]\n","        #l2s_test[index,2,:,1]=dataset[i]['there'][1]['->'][:flow_size]\n","        #l2s_test[index,3,:,1]=dataset[i]['here'][1]['<-'][:flow_size]\n","        labels_test[index] = 1\n","\n","        index += 1\n","    return l2s, labels, l2s_test, labels_test"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":58,"source":["def model(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","    flat_layers_after = [flow_size * 2, 1000, 50, 1]\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":59,"source":["def model_cnn(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","\n","    CNN_LAYERS = [[2, 20, 1, 2000, 5], [4, 10, 2000, 800, 3]]\n","\n","    for cnn_size in range(len(CNN_LAYERS)):\n","        cnn_weights = tf.get_variable(\n","            \"cnn_weight%d\" % cnn_size,\n","            CNN_LAYERS[cnn_size][:-1],\n","            initializer=tf.random_normal_initializer(stddev=0.01))\n","        cnn_bias = tf.get_variable(\"cnn_bias%d\" % cnn_size,\n","                                   [CNN_LAYERS[cnn_size][-2]],\n","                                   initializer=tf.zeros_initializer())\n","\n","        _x = tf.nn.conv2d(last_layer,\n","                          cnn_weights,\n","                          strides=[1, 2, 2, 1],\n","                          padding='VALID')\n","        _x = tf.nn.bias_add(_x, cnn_bias)\n","        conv = tf.nn.relu(_x, name='relu_cnn_%d' % cnn_size)\n","        pool = tf.nn.max_pool(conv,\n","                              ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1],\n","                              strides=[1, 1, 1, 1],\n","                              padding='VALID')\n","        last_layer = pool\n","    last_layer = tf.reshape(last_layer, [batch_size, -1])\n","\n","    flat_layers_after = [49600, 3000, 800, 100, 1]\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":60,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"error","ename":"TypeError","evalue":"Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=362'>363</a>\u001b[0m         \u001b[0;31m# Look up embeddings for inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=363'>364</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=364'>365</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=365'>366</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=366'>367</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36mmodel_cnn\u001b[0;34m(flow_before, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=274'>275</a>\u001b[0m                               padding='VALID')\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=275'>276</a>\u001b[0m         \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=276'>277</a>\u001b[0;31m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=277'>278</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=278'>279</a>\u001b[0m     \u001b[0mflat_layers_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m49600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6198\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6199\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   6200\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64"]}],"metadata":{}},{"cell_type":"code","execution_count":61,"source":["def model_cnn(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","\n","    CNN_LAYERS = [[2, 20, 1, 2000, 5], [4, 10, 2000, 800, 3]]\n","\n","    for cnn_size in range(len(CNN_LAYERS)):\n","        cnn_weights = tf.get_variable(\n","            \"cnn_weight%d\" % cnn_size,\n","            CNN_LAYERS[cnn_size][:-1],\n","            initializer=tf.random_normal_initializer(stddev=0.01))\n","        cnn_bias = tf.get_variable(\"cnn_bias%d\" % cnn_size,\n","                                   [CNN_LAYERS[cnn_size][-2]],\n","                                   initializer=tf.zeros_initializer())\n","\n","        _x = tf.nn.conv2d(last_layer,\n","                          cnn_weights,\n","                          strides=[1, 2, 2, 1],\n","                          padding='VALID')\n","        _x = tf.nn.bias_add(_x, cnn_bias)\n","        conv = tf.nn.relu(_x, name='relu_cnn_%d' % cnn_size)\n","        pool = tf.nn.max_pool(conv,\n","                              ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1],\n","                              strides=[1, 1, 1, 1],\n","                              padding='VALID')\n","        last_layer = pool\n","    last_layer = tf.reshape(int(last_layer), [batch_size, -1])\n","\n","    flat_layers_after = [49600, 3000, 800, 100, 1]\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":62,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"error","ename":"TypeError","evalue":"int() argument must be a string, a bytes-like object or a number, not 'Tensor'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=362'>363</a>\u001b[0m         \u001b[0;31m# Look up embeddings for inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=363'>364</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=364'>365</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=365'>366</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=366'>367</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36mmodel_cnn\u001b[0;34m(flow_before, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=274'>275</a>\u001b[0m                               padding='VALID')\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=275'>276</a>\u001b[0m         \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=276'>277</a>\u001b[0;31m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=277'>278</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=278'>279</a>\u001b[0m     \u001b[0mflat_layers_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m49600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'Tensor'"]}],"metadata":{}},{"cell_type":"code","execution_count":63,"source":["def model_cnn(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","\n","    CNN_LAYERS = [[2, 20, 1, 2000, 5], [4, 10, 2000, 800, 3]]\n","\n","    for cnn_size in range(len(CNN_LAYERS)):\n","        cnn_weights = tf.get_variable(\n","            \"cnn_weight%d\" % cnn_size,\n","            CNN_LAYERS[cnn_size][:-1],\n","            initializer=tf.random_normal_initializer(stddev=0.01))\n","        cnn_bias = tf.get_variable(\"cnn_bias%d\" % cnn_size,\n","                                   [CNN_LAYERS[cnn_size][-2]],\n","                                   initializer=tf.zeros_initializer())\n","\n","        _x = tf.nn.conv2d(last_layer,\n","                          cnn_weights,\n","                          strides=[1, 2, 2, 1],\n","                          padding='VALID')\n","        _x = tf.nn.bias_add(_x, cnn_bias)\n","        conv = tf.nn.relu(_x, name='relu_cnn_%d' % cnn_size)\n","        pool = tf.nn.max_pool(conv,\n","                              ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1],\n","                              strides=[1, 1, 1, 1],\n","                              padding='VALID')\n","        last_layer = pool\n","    last_layer = int(tf.reshape(last_layer, [batch_size, -1]))\n","\n","    flat_layers_after = [49600, 3000, 800, 100, 1]\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":64,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"error","ename":"TypeError","evalue":"Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=362'>363</a>\u001b[0m         \u001b[0;31m# Look up embeddings for inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=363'>364</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=364'>365</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=365'>366</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=366'>367</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36mmodel_cnn\u001b[0;34m(flow_before, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=274'>275</a>\u001b[0m                               padding='VALID')\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=275'>276</a>\u001b[0m         \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=276'>277</a>\u001b[0;31m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=277'>278</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=278'>279</a>\u001b[0m     \u001b[0mflat_layers_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m49600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6198\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6199\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   6200\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64"]}],"metadata":{}},{"cell_type":"code","execution_count":65,"source":["def model_cnn(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","\n","    CNN_LAYERS = [[2, 20, 1, 2000, 5], [4, 10, 2000, 800, 3]]\n","\n","    for cnn_size in range(len(CNN_LAYERS)):\n","        cnn_weights = tf.get_variable(\n","            \"cnn_weight%d\" % cnn_size,\n","            CNN_LAYERS[cnn_size][:-1],\n","            initializer=tf.random_normal_initializer(stddev=0.01))\n","        cnn_bias = tf.get_variable(\"cnn_bias%d\" % cnn_size,\n","                                   [CNN_LAYERS[cnn_size][-2]],\n","                                   initializer=tf.zeros_initializer())\n","\n","        _x = tf.nn.conv2d(last_layer,\n","                          cnn_weights,\n","                          strides=[1, 2, 2, 1],\n","                          padding='VALID')\n","        _x = tf.nn.bias_add(_x, cnn_bias)\n","        conv = tf.nn.relu(_x, name='relu_cnn_%d' % cnn_size)\n","        pool = tf.nn.max_pool(conv,\n","                              ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1],\n","                              strides=[1, 1, 1, 1],\n","                              padding='VALID')\n","        last_layer = pool\n","    last_layer = tf.reshape(last_layer, [batch_size, -1])\n","\n","    flat_layers_after = [49600, 3000, 800, 100, 1]\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":66,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"error","ename":"TypeError","evalue":"Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=362'>363</a>\u001b[0m         \u001b[0;31m# Look up embeddings for inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=363'>364</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=364'>365</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=365'>366</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=366'>367</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36mmodel_cnn\u001b[0;34m(flow_before, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=274'>275</a>\u001b[0m                               padding='VALID')\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=275'>276</a>\u001b[0m         \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=276'>277</a>\u001b[0;31m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=277'>278</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=278'>279</a>\u001b[0m     \u001b[0mflat_layers_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m49600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6198\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6199\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   6200\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64"]}],"metadata":{}},{"cell_type":"code","execution_count":67,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        print(train_flow_before)\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"flow_before_placeholder:0\", shape=(1402, 8, 300, 1), dtype=float32)\n"]},{"output_type":"error","ename":"TypeError","evalue":"Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=363'>364</a>\u001b[0m         \u001b[0;31m# Look up embeddings for inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=364'>365</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=365'>366</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=366'>367</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=367'>368</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36mmodel_cnn\u001b[0;34m(flow_before, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=274'>275</a>\u001b[0m                               padding='VALID')\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=275'>276</a>\u001b[0m         \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=276'>277</a>\u001b[0;31m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=277'>278</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=278'>279</a>\u001b[0m     \u001b[0mflat_layers_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m49600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6198\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6199\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   6200\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64"]}],"metadata":{}},{"cell_type":"code","execution_count":68,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        print(train_flow_before)\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","        tf.cast(train_flow_before, tf.int32)\n","        tf.cast(dropout_keep_prob, tf.int32)\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"flow_before_placeholder:0\", shape=(1402, 8, 300, 1), dtype=float32)\n"]},{"output_type":"error","ename":"TypeError","evalue":"Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=364'>365</a>\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=365'>366</a>\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=366'>367</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=367'>368</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=368'>369</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36mmodel_cnn\u001b[0;34m(flow_before, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=274'>275</a>\u001b[0m                               padding='VALID')\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=275'>276</a>\u001b[0m         \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=276'>277</a>\u001b[0;31m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=277'>278</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=278'>279</a>\u001b[0m     \u001b[0mflat_layers_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m49600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6198\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6199\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   6200\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64"]}],"metadata":{}},{"cell_type":"code","execution_count":69,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        print(train_flow_before)\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","        tf.cast(train_flow_before, tf.int32)\n","        tf.cast(dropout_keep_prob, tf.int32)\n","        print(train_flow_before)\n","        print(dropout_keep_prob)\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"flow_before_placeholder:0\", shape=(1402, 8, 300, 1), dtype=float32)\n","Tensor(\"flow_before_placeholder:0\", shape=(1402, 8, 300, 1), dtype=float32)\n","Tensor(\"dropout_placeholder:0\", dtype=float32)\n"]},{"output_type":"error","ename":"TypeError","evalue":"Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=367'>368</a>\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=368'>369</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=369'>370</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=370'>371</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=371'>372</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36mmodel_cnn\u001b[0;34m(flow_before, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=274'>275</a>\u001b[0m                               padding='VALID')\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=275'>276</a>\u001b[0m         \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=276'>277</a>\u001b[0;31m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=277'>278</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=278'>279</a>\u001b[0m     \u001b[0mflat_layers_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m49600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6198\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6199\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   6200\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64"]}],"metadata":{}},{"cell_type":"code","execution_count":70,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        print(train_flow_before)\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","        train_flow_before = tf.cast(train_flow_before, tf.int32)\n","        dropout_keep_prob = tf.cast(dropout_keep_prob, tf.int32)\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"flow_before_placeholder:0\", shape=(1402, 8, 300, 1), dtype=float32)\n"]},{"output_type":"error","ename":"TypeError","evalue":"Value passed to parameter 'input' has DataType int32 not in list of allowed values: float16, bfloat16, float32, float64","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=365'>366</a>\u001b[0m         \u001b[0mdropout_keep_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=366'>367</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=367'>368</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=368'>369</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=369'>370</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36mmodel_cnn\u001b[0;34m(flow_before, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=266'>267</a>\u001b[0m                           \u001b[0mcnn_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=267'>268</a>\u001b[0m                           \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=268'>269</a>\u001b[0;31m                           padding='VALID')\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=269'>270</a>\u001b[0m         \u001b[0m_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=270'>271</a>\u001b[0m         \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu_cnn_%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcnn_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;34m\"Conv2D\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m         data_format=data_format, dilations=dilations, name=name)\n\u001b[0m\u001b[1;32m    957\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'input' has DataType int32 not in list of allowed values: float16, bfloat16, float32, float64"]}],"metadata":{}},{"cell_type":"code","execution_count":71,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.int32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        print(train_flow_before)\n","        train_label = tf.placeholder(tf.int32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.int32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","        # train_flow_before = tf.cast(train_flow_before, tf.int32)\n","        # dropout_keep_prob = tf.cast(dropout_keep_prob, tf.int32)\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"flow_before_placeholder:0\", shape=(1402, 8, 300, 1), dtype=int32)\n"]},{"output_type":"error","ename":"TypeError","evalue":"Value passed to parameter 'input' has DataType int32 not in list of allowed values: float16, bfloat16, float32, float64","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=364'>365</a>\u001b[0m         \u001b[0;31m# train_flow_before = tf.cast(train_flow_before, tf.int32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=365'>366</a>\u001b[0m         \u001b[0;31m# dropout_keep_prob = tf.cast(dropout_keep_prob, tf.int32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=366'>367</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=367'>368</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=368'>369</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36mmodel_cnn\u001b[0;34m(flow_before, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=266'>267</a>\u001b[0m                           \u001b[0mcnn_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=267'>268</a>\u001b[0m                           \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=268'>269</a>\u001b[0;31m                           padding='VALID')\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=269'>270</a>\u001b[0m         \u001b[0m_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=270'>271</a>\u001b[0m         \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu_cnn_%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcnn_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;34m\"Conv2D\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m         data_format=data_format, dilations=dilations, name=name)\n\u001b[0m\u001b[1;32m    957\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'input' has DataType int32 not in list of allowed values: float16, bfloat16, float32, float64"]}],"metadata":{}},{"cell_type":"code","execution_count":72,"source":["def model_cnn(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","\n","    CNN_LAYERS = [[2, 20, 1, 2000, 5], [4, 10, 2000, 800, 3]]\n","\n","    for cnn_size in range(len(CNN_LAYERS)):\n","        cnn_weights = tf.get_variable(\n","            \"cnn_weight%d\" % cnn_size,\n","            CNN_LAYERS[cnn_size][:-1],\n","            initializer=tf.random_normal_initializer(stddev=0.01))\n","        cnn_bias = tf.get_variable(\"cnn_bias%d\" % cnn_size,\n","                                   [CNN_LAYERS[cnn_size][-2]],\n","                                   initializer=tf.zeros_initializer())\n","\n","        _x = tf.nn.conv2d(last_layer,\n","                          cnn_weights,\n","                          strides=[1, 2, 2, 1],\n","                          padding='VALID')\n","        _x = tf.nn.bias_add(_x, cnn_bias)\n","        conv = tf.nn.relu(_x, name='relu_cnn_%d' % cnn_size)\n","        pool = tf.nn.max_pool(conv,\n","                              ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1],\n","                              strides=[1, 1, 1, 1],\n","                              padding='VALID')\n","        last_layer = pool\n","    last_layer_int = tf.cast(last_layer, dtype=tf.int32)\n","    last_layer = tf.reshape(last_layer_int, [batch_size, -1])\n","\n","    flat_layers_after = [49600, 3000, 800, 100, 1]\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":73,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        print(train_flow_before)\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","        # train_flow_before = tf.cast(train_flow_before, tf.int32)\n","        # dropout_keep_prob = tf.cast(dropout_keep_prob, tf.int32)\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"flow_before_placeholder:0\", shape=(1402, 8, 300, 1), dtype=float32)\n"]},{"output_type":"error","ename":"TypeError","evalue":"Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=365'>366</a>\u001b[0m         \u001b[0;31m# train_flow_before = tf.cast(train_flow_before, tf.int32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=366'>367</a>\u001b[0m         \u001b[0;31m# dropout_keep_prob = tf.cast(dropout_keep_prob, tf.int32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=367'>368</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=368'>369</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=369'>370</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36mmodel_cnn\u001b[0;34m(flow_before, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=275'>276</a>\u001b[0m         \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=276'>277</a>\u001b[0m     \u001b[0mlast_layer_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=277'>278</a>\u001b[0;31m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=278'>279</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=279'>280</a>\u001b[0m     \u001b[0mflat_layers_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m49600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6198\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6199\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   6200\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64"]}],"metadata":{}},{"cell_type":"code","execution_count":74,"source":["def model_cnn(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","\n","    CNN_LAYERS = [[2, 20, 1, 2000, 5], [4, 10, 2000, 800, 3]]\n","\n","    for cnn_size in range(len(CNN_LAYERS)):\n","        cnn_weights = tf.get_variable(\n","            \"cnn_weight%d\" % cnn_size,\n","            CNN_LAYERS[cnn_size][:-1],\n","            initializer=tf.random_normal_initializer(stddev=0.01))\n","        cnn_bias = tf.get_variable(\"cnn_bias%d\" % cnn_size,\n","                                   [CNN_LAYERS[cnn_size][-2]],\n","                                   initializer=tf.zeros_initializer())\n","\n","        _x = tf.nn.conv2d(last_layer,\n","                          cnn_weights,\n","                          strides=[1, 2, 2, 1],\n","                          padding='VALID')\n","        _x = tf.nn.bias_add(_x, cnn_bias)\n","        conv = tf.nn.relu(_x, name='relu_cnn_%d' % cnn_size)\n","        pool = tf.nn.max_pool(conv,\n","                              ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1],\n","                              strides=[1, 1, 1, 1],\n","                              padding='VALID')\n","        last_layer = pool\n","    last_layer = tf.reshape(last_layer, [batch_size, -1])\n","\n","    flat_layers_after = [49600, 3000, 800, 100, 1]\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    last_layer = tf.cast(last_layer, dtype=tf.int32)\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":75,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        print(train_flow_before)\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","        # train_flow_before = tf.cast(train_flow_before, tf.int32)\n","        # dropout_keep_prob = tf.cast(dropout_keep_prob, tf.int32)\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"flow_before_placeholder:0\", shape=(1402, 8, 300, 1), dtype=float32)\n"]},{"output_type":"error","ename":"TypeError","evalue":"Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=365'>366</a>\u001b[0m         \u001b[0;31m# train_flow_before = tf.cast(train_flow_before, tf.int32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=366'>367</a>\u001b[0m         \u001b[0;31m# dropout_keep_prob = tf.cast(dropout_keep_prob, tf.int32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=367'>368</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=368'>369</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=369'>370</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36mmodel_cnn\u001b[0;34m(flow_before, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=274'>275</a>\u001b[0m                               padding='VALID')\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=275'>276</a>\u001b[0m         \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=276'>277</a>\u001b[0;31m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=277'>278</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=278'>279</a>\u001b[0m     \u001b[0mflat_layers_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m49600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6198\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6199\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   6200\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64"]}],"metadata":{}},{"cell_type":"code","execution_count":76,"source":["def model_cnn(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","\n","    CNN_LAYERS = [[2, 20, 1, 2000, 5], [4, 10, 2000, 800, 3]]\n","\n","    for cnn_size in range(len(CNN_LAYERS)):\n","        cnn_weights = tf.get_variable(\n","            \"cnn_weight%d\" % cnn_size,\n","            CNN_LAYERS[cnn_size][:-1],\n","            initializer=tf.random_normal_initializer(stddev=0.01))\n","        cnn_bias = tf.get_variable(\"cnn_bias%d\" % cnn_size,\n","                                   [CNN_LAYERS[cnn_size][-2]],\n","                                   initializer=tf.zeros_initializer())\n","\n","        _x = tf.nn.conv2d(last_layer,\n","                          cnn_weights,\n","                          strides=[1, 2, 2, 1],\n","                          padding='VALID')\n","        _x = tf.nn.bias_add(_x, cnn_bias)\n","        conv = tf.nn.relu(_x, name='relu_cnn_%d' % cnn_size)\n","        pool = tf.nn.max_pool(conv,\n","                              ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1],\n","                              strides=[1, 1, 1, 1],\n","                              padding='VALID')\n","        last_layer = pool\n","    last_layer = tf.cast(last_layer, dtype=tf.int32)\n","    last_layer = tf.reshape(last_layer, [batch_size, -1])\n","\n","    flat_layers_after = [49600, 3000, 800, 100, 1]\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    last_layer = tf.cast(last_layer, dtype=tf.int32)\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":77,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        print(train_flow_before)\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","        # train_flow_before = tf.cast(train_flow_before, tf.int32)\n","        # dropout_keep_prob = tf.cast(dropout_keep_prob, tf.int32)\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"flow_before_placeholder:0\", shape=(1402, 8, 300, 1), dtype=float32)\n"]},{"output_type":"error","ename":"TypeError","evalue":"Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=366'>367</a>\u001b[0m         \u001b[0;31m# train_flow_before = tf.cast(train_flow_before, tf.int32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=367'>368</a>\u001b[0m         \u001b[0;31m# dropout_keep_prob = tf.cast(dropout_keep_prob, tf.int32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=368'>369</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=369'>370</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=370'>371</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36mmodel_cnn\u001b[0;34m(flow_before, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=275'>276</a>\u001b[0m         \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=276'>277</a>\u001b[0m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=277'>278</a>\u001b[0;31m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=278'>279</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=279'>280</a>\u001b[0m     \u001b[0mflat_layers_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m49600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6198\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6199\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   6200\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64"]}],"metadata":{}},{"cell_type":"code","execution_count":78,"source":["def model_cnn(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","\n","    CNN_LAYERS = [[2, 20, 1, 2000, 5], [4, 10, 2000, 800, 3]]\n","\n","    for cnn_size in range(len(CNN_LAYERS)):\n","        cnn_weights = tf.get_variable(\n","            \"cnn_weight%d\" % cnn_size,\n","            CNN_LAYERS[cnn_size][:-1],\n","            initializer=tf.random_normal_initializer(stddev=0.01))\n","        cnn_bias = tf.get_variable(\"cnn_bias%d\" % cnn_size,\n","                                   [CNN_LAYERS[cnn_size][-2]],\n","                                   initializer=tf.zeros_initializer())\n","\n","        _x = tf.nn.conv2d(last_layer,\n","                          cnn_weights,\n","                          strides=[1, 2, 2, 1],\n","                          padding='VALID')\n","        _x = tf.nn.bias_add(_x, cnn_bias)\n","        conv = tf.nn.relu(_x, name='relu_cnn_%d' % cnn_size)\n","        pool = tf.nn.max_pool(conv,\n","                              ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1],\n","                              strides=[1, 1, 1, 1],\n","                              padding='VALID')\n","        last_layer = pool\n","    last_layer = tf.cast(last_layer, dtype=tf.int32)\n","    print(last_layer)\n","    last_layer = tf.reshape(last_layer, [batch_size, -1])\n","\n","    flat_layers_after = [49600, 3000, 800, 100, 1]\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    last_layer = tf.cast(last_layer, dtype=tf.int32)\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":79,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        print(train_flow_before)\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","        # train_flow_before = tf.cast(train_flow_before, tf.int32)\n","        # dropout_keep_prob = tf.cast(dropout_keep_prob, tf.int32)\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"flow_before_placeholder:0\", shape=(1402, 8, 300, 1), dtype=float32)\n","Tensor(\"Cast:0\", shape=(1402, 1, 62, 800), dtype=int32)\n"]},{"output_type":"error","ename":"TypeError","evalue":"Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=367'>368</a>\u001b[0m         \u001b[0;31m# train_flow_before = tf.cast(train_flow_before, tf.int32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=368'>369</a>\u001b[0m         \u001b[0;31m# dropout_keep_prob = tf.cast(dropout_keep_prob, tf.int32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=369'>370</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=370'>371</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=371'>372</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36mmodel_cnn\u001b[0;34m(flow_before, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=276'>277</a>\u001b[0m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=277'>278</a>\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=278'>279</a>\u001b[0;31m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=279'>280</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=280'>281</a>\u001b[0m     \u001b[0mflat_layers_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m49600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6198\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6199\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   6200\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64"]}],"metadata":{}},{"cell_type":"code","execution_count":80,"source":["def model_cnn(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","\n","    CNN_LAYERS = [[2, 20, 1, 2000, 5], [4, 10, 2000, 800, 3]]\n","\n","    for cnn_size in range(len(CNN_LAYERS)):\n","        cnn_weights = tf.get_variable(\n","            \"cnn_weight%d\" % cnn_size,\n","            CNN_LAYERS[cnn_size][:-1],\n","            initializer=tf.random_normal_initializer(stddev=0.01))\n","        cnn_bias = tf.get_variable(\"cnn_bias%d\" % cnn_size,\n","                                   [CNN_LAYERS[cnn_size][-2]],\n","                                   initializer=tf.zeros_initializer())\n","\n","        _x = tf.nn.conv2d(last_layer,\n","                          cnn_weights,\n","                          strides=[1, 2, 2, 1],\n","                          padding='VALID')\n","        _x = tf.nn.bias_add(_x, cnn_bias)\n","        conv = tf.nn.relu(_x, name='relu_cnn_%d' % cnn_size)\n","        pool = tf.nn.max_pool(conv,\n","                              ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1],\n","                              strides=[1, 1, 1, 1],\n","                              padding='VALID')\n","        last_layer = pool\n","    last_layer = tf.cast(last_layer, dtype=tf.int32)\n","    print(last_layer)\n","    last_layer = tf.reshape(last_layer, [batch_size, -1])\n","\n","    flat_layers_after = [49600, 3000, 800, 100, 1]\n","    print(flat_layers_after)\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    last_layer = tf.cast(last_layer, dtype=tf.int32)\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":81,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        print(train_flow_before)\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","        # train_flow_before = tf.cast(train_flow_before, tf.int32)\n","        # dropout_keep_prob = tf.cast(dropout_keep_prob, tf.int32)\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"flow_before_placeholder:0\", shape=(1402, 8, 300, 1), dtype=float32)\n","Tensor(\"Cast:0\", shape=(1402, 1, 62, 800), dtype=int32)\n"]},{"output_type":"error","ename":"TypeError","evalue":"Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=368'>369</a>\u001b[0m         \u001b[0;31m# train_flow_before = tf.cast(train_flow_before, tf.int32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=369'>370</a>\u001b[0m         \u001b[0;31m# dropout_keep_prob = tf.cast(dropout_keep_prob, tf.int32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=370'>371</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=371'>372</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=372'>373</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36mmodel_cnn\u001b[0;34m(flow_before, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=276'>277</a>\u001b[0m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=277'>278</a>\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=278'>279</a>\u001b[0;31m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=279'>280</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=280'>281</a>\u001b[0m     \u001b[0mflat_layers_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m49600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6198\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6199\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   6200\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64"]}],"metadata":{}},{"cell_type":"code","execution_count":82,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        print(train_flow_before)\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","        train_flow_before = tf.cast(train_flow_before, dtype=tf.int32)\n","        dropout_keep_prob = tf.cast(dropout_keep_prob, dtype=tf.int32)\n","        print(train_flow_before)\n","        print(dropout_keep_prob)\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"flow_before_placeholder:0\", shape=(1402, 8, 300, 1), dtype=float32)\n","Tensor(\"Cast:0\", shape=(1402, 8, 300, 1), dtype=int32)\n","Tensor(\"Cast_1:0\", dtype=int32)\n"]},{"output_type":"error","ename":"TypeError","evalue":"Value passed to parameter 'input' has DataType int32 not in list of allowed values: float16, bfloat16, float32, float64","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=370'>371</a>\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=371'>372</a>\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=372'>373</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=373'>374</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=374'>375</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36mmodel_cnn\u001b[0;34m(flow_before, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=266'>267</a>\u001b[0m                           \u001b[0mcnn_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=267'>268</a>\u001b[0m                           \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=268'>269</a>\u001b[0;31m                           padding='VALID')\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=269'>270</a>\u001b[0m         \u001b[0m_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=270'>271</a>\u001b[0m         \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu_cnn_%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcnn_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;34m\"Conv2D\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m         data_format=data_format, dilations=dilations, name=name)\n\u001b[0m\u001b[1;32m    957\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'input' has DataType int32 not in list of allowed values: float16, bfloat16, float32, float64"]}],"metadata":{}},{"cell_type":"code","execution_count":83,"source":["def model_cnn(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","    last_layer = tf.cast(last_layer, dtype=tf.float32)\n","    dropout_keep_prob = tf.cast(dropout_keep_prob, dtype=tf.float32)\n","\n","    CNN_LAYERS = [[2, 20, 1, 2000, 5], [4, 10, 2000, 800, 3]]\n","\n","    for cnn_size in range(len(CNN_LAYERS)):\n","        cnn_weights = tf.get_variable(\n","            \"cnn_weight%d\" % cnn_size,\n","            CNN_LAYERS[cnn_size][:-1],\n","            initializer=tf.random_normal_initializer(stddev=0.01))\n","        cnn_bias = tf.get_variable(\"cnn_bias%d\" % cnn_size,\n","                                   [CNN_LAYERS[cnn_size][-2]],\n","                                   initializer=tf.zeros_initializer())\n","\n","        _x = tf.nn.conv2d(last_layer,\n","                          cnn_weights,\n","                          strides=[1, 2, 2, 1],\n","                          padding='VALID')\n","        _x = tf.nn.bias_add(_x, cnn_bias)\n","        conv = tf.nn.relu(_x, name='relu_cnn_%d' % cnn_size)\n","        pool = tf.nn.max_pool(conv,\n","                              ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1],\n","                              strides=[1, 1, 1, 1],\n","                              padding='VALID')\n","        last_layer = pool\n","    last_layer = tf.cast(last_layer, dtype=tf.int32)\n","    print(last_layer)\n","    last_layer = tf.reshape(last_layer, [batch_size, -1])\n","\n","    flat_layers_after = [49600, 3000, 800, 100, 1]\n","    print(flat_layers_after)\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    last_layer = tf.cast(last_layer, dtype=tf.int32)\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":84,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        print(train_flow_before)\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","        train_flow_before = tf.cast(train_flow_before, dtype=tf.int32)\n","        dropout_keep_prob = tf.cast(dropout_keep_prob, dtype=tf.int32)\n","        print(train_flow_before)\n","        print(dropout_keep_prob)\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"flow_before_placeholder:0\", shape=(1402, 8, 300, 1), dtype=float32)\n","Tensor(\"Cast:0\", shape=(1402, 8, 300, 1), dtype=int32)\n","Tensor(\"Cast_1:0\", dtype=int32)\n","Tensor(\"Cast_4:0\", shape=(1402, 1, 62, 800), dtype=int32)\n"]},{"output_type":"error","ename":"TypeError","evalue":"Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=372'>373</a>\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=373'>374</a>\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=374'>375</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=375'>376</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=376'>377</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36mmodel_cnn\u001b[0;34m(flow_before, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=278'>279</a>\u001b[0m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=279'>280</a>\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=280'>281</a>\u001b[0;31m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=281'>282</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=282'>283</a>\u001b[0m     \u001b[0mflat_layers_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m49600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6198\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6199\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   6200\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64"]}],"metadata":{}},{"cell_type":"code","execution_count":85,"source":["def model_cnn(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","    last_layer = tf.cast(last_layer, dtype=tf.float32)\n","    dropout_keep_prob = tf.cast(dropout_keep_prob, dtype=tf.float32)\n","\n","    CNN_LAYERS = [[2, 20, 1, 2000, 5], [4, 10, 2000, 800, 3]]\n","\n","    for cnn_size in range(len(CNN_LAYERS)):\n","        cnn_weights = tf.get_variable(\n","            \"cnn_weight%d\" % cnn_size,\n","            CNN_LAYERS[cnn_size][:-1],\n","            initializer=tf.random_normal_initializer(stddev=0.01))\n","        cnn_bias = tf.get_variable(\"cnn_bias%d\" % cnn_size,\n","                                   [CNN_LAYERS[cnn_size][-2]],\n","                                   initializer=tf.zeros_initializer())\n","\n","        _x = tf.nn.conv2d(last_layer,\n","                          cnn_weights,\n","                          strides=[1, 2, 2, 1],\n","                          padding='VALID')\n","        _x = tf.nn.bias_add(_x, cnn_bias)\n","        conv = tf.nn.relu(_x, name='relu_cnn_%d' % cnn_size)\n","        pool = tf.nn.max_pool(conv,\n","                              ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1],\n","                              strides=[1, 1, 1, 1],\n","                              padding='VALID')\n","        last_layer = pool\n","    last_layer = tf.reshape(last_layer, [batch_size, -1])\n","\n","    flat_layers_after = [49600, 3000, 800, 100, 1]\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    last_layer = tf.cast(last_layer, dtype=tf.int32)\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":86,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        print(train_flow_before)\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","        train_flow_before = tf.cast(train_flow_before, dtype=tf.int32)\n","        dropout_keep_prob = tf.cast(dropout_keep_prob, dtype=tf.int32)\n","        print(train_flow_before)\n","        print(dropout_keep_prob)\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"flow_before_placeholder:0\", shape=(1402, 8, 300, 1), dtype=float32)\n","Tensor(\"Cast:0\", shape=(1402, 8, 300, 1), dtype=int32)\n","Tensor(\"Cast_1:0\", dtype=int32)\n"]},{"output_type":"error","ename":"TypeError","evalue":"Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=369'>370</a>\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=370'>371</a>\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=371'>372</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=372'>373</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=373'>374</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36mmodel_cnn\u001b[0;34m(flow_before, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=276'>277</a>\u001b[0m                               padding='VALID')\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=277'>278</a>\u001b[0m         \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=278'>279</a>\u001b[0;31m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=279'>280</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=280'>281</a>\u001b[0m     \u001b[0mflat_layers_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m49600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6198\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6199\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   6200\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64"]}],"metadata":{}},{"cell_type":"code","execution_count":87,"source":["def model_cnn(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","    # last_layer = tf.cast(last_layer, dtype=tf.float32)\n","    # dropout_keep_prob = tf.cast(dropout_keep_prob, dtype=tf.float32)\n","\n","    CNN_LAYERS = [[2, 20, 1, 2000, 5], [4, 10, 2000, 800, 3]]\n","\n","    for cnn_size in range(len(CNN_LAYERS)):\n","        cnn_weights = tf.get_variable(\n","            \"cnn_weight%d\" % cnn_size,\n","            CNN_LAYERS[cnn_size][:-1],\n","            initializer=tf.random_normal_initializer(stddev=0.01))\n","        cnn_bias = tf.get_variable(\"cnn_bias%d\" % cnn_size,\n","                                   [CNN_LAYERS[cnn_size][-2]],\n","                                   initializer=tf.zeros_initializer())\n","\n","        _x = tf.nn.conv2d(last_layer,\n","                          cnn_weights,\n","                          strides=[1, 2, 2, 1],\n","                          padding='VALID')\n","        _x = tf.nn.bias_add(_x, cnn_bias)\n","        conv = tf.nn.relu(_x, name='relu_cnn_%d' % cnn_size)\n","        pool = tf.nn.max_pool(conv,\n","                              ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1],\n","                              strides=[1, 1, 1, 1],\n","                              padding='VALID')\n","        last_layer = pool\n","    last_layer = tf.reshape(last_layer, [batch_size, -1])\n","\n","    flat_layers_after = [49600, 3000, 800, 100, 1]\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    last_layer = tf.cast(last_layer, dtype=tf.int32)\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":88,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        print(train_flow_before)\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","        train_flow_before = tf.cast(train_flow_before, dtype=tf.int32)\n","        dropout_keep_prob = tf.cast(dropout_keep_prob, dtype=tf.int32)\n","        print(train_flow_before)\n","        print(dropout_keep_prob)\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"flow_before_placeholder:0\", shape=(1402, 8, 300, 1), dtype=float32)\n","Tensor(\"Cast:0\", shape=(1402, 8, 300, 1), dtype=int32)\n","Tensor(\"Cast_1:0\", dtype=int32)\n"]},{"output_type":"error","ename":"TypeError","evalue":"Value passed to parameter 'input' has DataType int32 not in list of allowed values: float16, bfloat16, float32, float64","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=369'>370</a>\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=370'>371</a>\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=371'>372</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=372'>373</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=373'>374</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36mmodel_cnn\u001b[0;34m(flow_before, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=268'>269</a>\u001b[0m                           \u001b[0mcnn_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=269'>270</a>\u001b[0m                           \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=270'>271</a>\u001b[0;31m                           padding='VALID')\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=271'>272</a>\u001b[0m         \u001b[0m_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=272'>273</a>\u001b[0m         \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu_cnn_%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcnn_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;34m\"Conv2D\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m         data_format=data_format, dilations=dilations, name=name)\n\u001b[0m\u001b[1;32m    957\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'input' has DataType int32 not in list of allowed values: float16, bfloat16, float32, float64"]}],"metadata":{}},{"cell_type":"code","execution_count":89,"source":["def model_cnn(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","    # last_layer = tf.cast(last_layer, dtype=tf.float32)\n","    # dropout_keep_prob = tf.cast(dropout_keep_prob, dtype=tf.float32)\n","\n","    CNN_LAYERS = [[2, 20, 1, 2000, 5], [4, 10, 2000, 800, 3]]\n","\n","    for cnn_size in range(len(CNN_LAYERS)):\n","        cnn_weights = tf.get_variable(\n","            \"cnn_weight%d\" % cnn_size,\n","            CNN_LAYERS[cnn_size][:-1],\n","            initializer=tf.random_normal_initializer(stddev=0.01))\n","        cnn_bias = tf.get_variable(\"cnn_bias%d\" % cnn_size,\n","                                   [CNN_LAYERS[cnn_size][-2]],\n","                                   initializer=tf.zeros_initializer())\n","        last_layer = tf.cast(last_layer, dtype=tf.float32)\n","        _x = tf.nn.conv2d(last_layer,\n","                          cnn_weights,\n","                          strides=[1, 2, 2, 1],\n","                          padding='VALID')\n","        _x = tf.nn.bias_add(_x, cnn_bias)\n","        conv = tf.nn.relu(_x, name='relu_cnn_%d' % cnn_size)\n","        pool = tf.nn.max_pool(conv,\n","                              ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1],\n","                              strides=[1, 1, 1, 1],\n","                              padding='VALID')\n","        last_layer = pool\n","    last_layer = tf.reshape(last_layer, [batch_size, -1])\n","\n","    flat_layers_after = [49600, 3000, 800, 100, 1]\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    last_layer = tf.cast(last_layer, dtype=tf.int32)\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":90,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        print(train_flow_before)\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","        train_flow_before = tf.cast(train_flow_before, dtype=tf.int32)\n","        dropout_keep_prob = tf.cast(dropout_keep_prob, dtype=tf.int32)\n","        print(train_flow_before)\n","        print(dropout_keep_prob)\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"flow_before_placeholder:0\", shape=(1402, 8, 300, 1), dtype=float32)\n","Tensor(\"Cast:0\", shape=(1402, 8, 300, 1), dtype=int32)\n","Tensor(\"Cast_1:0\", dtype=int32)\n"]},{"output_type":"error","ename":"TypeError","evalue":"Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=369'>370</a>\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=370'>371</a>\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=371'>372</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=372'>373</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=373'>374</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36mmodel_cnn\u001b[0;34m(flow_before, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=276'>277</a>\u001b[0m                               padding='VALID')\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=277'>278</a>\u001b[0m         \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=278'>279</a>\u001b[0;31m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=279'>280</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=280'>281</a>\u001b[0m     \u001b[0mflat_layers_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m49600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6198\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6199\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   6200\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64"]}],"metadata":{}},{"cell_type":"code","execution_count":91,"source":["def model_cnn(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","    # last_layer = tf.cast(last_layer, dtype=tf.float32)\n","    # dropout_keep_prob = tf.cast(dropout_keep_prob, dtype=tf.float32)\n","\n","    CNN_LAYERS = [[2, 20, 1, 2000, 5], [4, 10, 2000, 800, 3]]\n","\n","    for cnn_size in range(len(CNN_LAYERS)):\n","        cnn_weights = tf.get_variable(\n","            \"cnn_weight%d\" % cnn_size,\n","            CNN_LAYERS[cnn_size][:-1],\n","            initializer=tf.random_normal_initializer(stddev=0.01))\n","        cnn_bias = tf.get_variable(\"cnn_bias%d\" % cnn_size,\n","                                   [CNN_LAYERS[cnn_size][-2]],\n","                                   initializer=tf.zeros_initializer())\n","        last_layer = tf.cast(last_layer, dtype=tf.float32)\n","        _x = tf.nn.conv2d(last_layer,\n","                          cnn_weights,\n","                          strides=[1, 2, 2, 1],\n","                          padding='VALID')\n","        _x = tf.nn.bias_add(_x, cnn_bias)\n","        conv = tf.nn.relu(_x, name='relu_cnn_%d' % cnn_size)\n","        pool = tf.nn.max_pool(conv,\n","                              ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1],\n","                              strides=[1, 1, 1, 1],\n","                              padding='VALID')\n","        last_layer = pool\n","    last_layer = tf.cast(last_layer, dtype=tf.int32)\n","    last_layer = tf.reshape(last_layer, [batch_size, -1])\n","\n","    flat_layers_after = [49600, 3000, 800, 100, 1]\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    last_layer = tf.cast(last_layer, dtype=tf.int32)\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":92,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        print(train_flow_before)\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","        train_flow_before = tf.cast(train_flow_before, dtype=tf.int32)\n","        dropout_keep_prob = tf.cast(dropout_keep_prob, dtype=tf.int32)\n","        print(train_flow_before)\n","        print(dropout_keep_prob)\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"flow_before_placeholder:0\", shape=(1402, 8, 300, 1), dtype=float32)\n","Tensor(\"Cast:0\", shape=(1402, 8, 300, 1), dtype=int32)\n","Tensor(\"Cast_1:0\", dtype=int32)\n"]},{"output_type":"error","ename":"TypeError","evalue":"Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=370'>371</a>\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=371'>372</a>\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=372'>373</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=373'>374</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=374'>375</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36mmodel_cnn\u001b[0;34m(flow_before, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=277'>278</a>\u001b[0m         \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=278'>279</a>\u001b[0m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=279'>280</a>\u001b[0;31m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=280'>281</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=281'>282</a>\u001b[0m     \u001b[0mflat_layers_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m49600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6198\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6199\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   6200\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64"]}],"metadata":{}},{"cell_type":"code","execution_count":93,"source":["def model_cnn(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","    # last_layer = tf.cast(last_layer, dtype=tf.float32)\n","    # dropout_keep_prob = tf.cast(dropout_keep_prob, dtype=tf.float32)\n","\n","    CNN_LAYERS = [[2, 20, 1, 2000, 5], [4, 10, 2000, 800, 3]]\n","\n","    for cnn_size in range(len(CNN_LAYERS)):\n","        cnn_weights = tf.get_variable(\n","            \"cnn_weight%d\" % cnn_size,\n","            CNN_LAYERS[cnn_size][:-1],\n","            initializer=tf.random_normal_initializer(stddev=0.01))\n","        cnn_bias = tf.get_variable(\"cnn_bias%d\" % cnn_size,\n","                                   [CNN_LAYERS[cnn_size][-2]],\n","                                   initializer=tf.zeros_initializer())\n","        last_layer = tf.cast(last_layer, dtype=tf.float32)\n","        _x = tf.nn.conv2d(last_layer,\n","                          cnn_weights,\n","                          strides=[1, 2, 2, 1],\n","                          padding='VALID')\n","        _x = tf.nn.bias_add(_x, cnn_bias)\n","        conv = tf.nn.relu(_x, name='relu_cnn_%d' % cnn_size)\n","        pool = tf.nn.max_pool(conv,\n","                              ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1],\n","                              strides=[1, 1, 1, 1],\n","                              padding='VALID')\n","        last_layer = pool\n","    last_layer = tf.cast(last_layer, dtype=tf.int32)\n","    print(3, last_layer)\n","    last_layer = tf.reshape(last_layer, [batch_size, -1])\n","\n","    flat_layers_after = [49600, 3000, 800, 100, 1]\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    last_layer = tf.cast(last_layer, dtype=tf.int32)\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":94,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        print(train_flow_before)\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","        train_flow_before = tf.cast(train_flow_before, dtype=tf.int32)\n","        dropout_keep_prob = tf.cast(dropout_keep_prob, dtype=tf.int32)\n","        print(train_flow_before)\n","        print(dropout_keep_prob)\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"flow_before_placeholder:0\", shape=(1402, 8, 300, 1), dtype=float32)\n","Tensor(\"Cast:0\", shape=(1402, 8, 300, 1), dtype=int32)\n","Tensor(\"Cast_1:0\", dtype=int32)\n","3 Tensor(\"Cast_3:0\", shape=(1402, 1, 62, 800), dtype=int32)\n"]},{"output_type":"error","ename":"TypeError","evalue":"Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=371'>372</a>\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=372'>373</a>\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=373'>374</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=374'>375</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=375'>376</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36mmodel_cnn\u001b[0;34m(flow_before, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=278'>279</a>\u001b[0m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=279'>280</a>\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=280'>281</a>\u001b[0;31m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=281'>282</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=282'>283</a>\u001b[0m     \u001b[0mflat_layers_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m49600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6198\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6199\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   6200\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64"]}],"metadata":{}},{"cell_type":"code","execution_count":95,"source":["def model_cnn(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","    # last_layer = tf.cast(last_layer, dtype=tf.float32)\n","    # dropout_keep_prob = tf.cast(dropout_keep_prob, dtype=tf.float32)\n","\n","    CNN_LAYERS = [[2, 20, 1, 2000, 5], [4, 10, 2000, 800, 3]]\n","\n","    for cnn_size in range(len(CNN_LAYERS)):\n","        cnn_weights = tf.get_variable(\n","            \"cnn_weight%d\" % cnn_size,\n","            CNN_LAYERS[cnn_size][:-1],\n","            initializer=tf.random_normal_initializer(stddev=0.01))\n","        cnn_bias = tf.get_variable(\"cnn_bias%d\" % cnn_size,\n","                                   [CNN_LAYERS[cnn_size][-2]],\n","                                   initializer=tf.zeros_initializer())\n","        last_layer = tf.cast(last_layer, dtype=tf.float32)\n","        _x = tf.nn.conv2d(last_layer,\n","                          cnn_weights,\n","                          strides=[1, 2, 2, 1],\n","                          padding='VALID')\n","        _x = tf.nn.bias_add(_x, cnn_bias)\n","        conv = tf.nn.relu(_x, name='relu_cnn_%d' % cnn_size)\n","        pool = tf.nn.max_pool(conv,\n","                              ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1],\n","                              strides=[1, 1, 1, 1],\n","                              padding='VALID')\n","        last_layer = pool\n","    last_layer = tf.cast(last_layer, dtype=tf.int32)\n","    print(3, last_layer)\n","    last_layer = tf.reshape(last_layer, [batch_size, -1])\n","    print(4, last_layer)\n","    flat_layers_after = [49600, 3000, 800, 100, 1]\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    last_layer = tf.cast(last_layer, dtype=tf.int32)\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":96,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        print(train_flow_before)\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","        train_flow_before = tf.cast(train_flow_before, dtype=tf.int32)\n","        dropout_keep_prob = tf.cast(dropout_keep_prob, dtype=tf.int32)\n","        print(train_flow_before)\n","        print(dropout_keep_prob)\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"flow_before_placeholder:0\", shape=(1402, 8, 300, 1), dtype=float32)\n","Tensor(\"Cast:0\", shape=(1402, 8, 300, 1), dtype=int32)\n","Tensor(\"Cast_1:0\", dtype=int32)\n","3 Tensor(\"Cast_3:0\", shape=(1402, 1, 62, 800), dtype=int32)\n"]},{"output_type":"error","ename":"TypeError","evalue":"Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=371'>372</a>\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=372'>373</a>\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=373'>374</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=374'>375</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=375'>376</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36mmodel_cnn\u001b[0;34m(flow_before, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=278'>279</a>\u001b[0m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=279'>280</a>\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=280'>281</a>\u001b[0;31m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=281'>282</a>\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=282'>283</a>\u001b[0m     \u001b[0mflat_layers_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m49600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6198\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6199\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   6200\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64"]}],"metadata":{}},{"cell_type":"code","execution_count":97,"source":["def model_cnn(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","\n","    CNN_LAYERS = [[2, 20, 1, 2000, 5], [4, 10, 2000, 800, 3]]\n","\n","    for cnn_size in range(len(CNN_LAYERS)):\n","        cnn_weights = tf.get_variable(\n","            \"cnn_weight%d\" % cnn_size,\n","            CNN_LAYERS[cnn_size][:-1],\n","            initializer=tf.random_normal_initializer(stddev=0.01))\n","        cnn_bias = tf.get_variable(\"cnn_bias%d\" % cnn_size,\n","                                   [CNN_LAYERS[cnn_size][-2]],\n","                                   initializer=tf.zeros_initializer())\n","\n","        _x = tf.nn.conv2d(last_layer,\n","                          cnn_weights,\n","                          strides=[1, 2, 2, 1],\n","                          padding='VALID')\n","        _x = tf.nn.bias_add(_x, cnn_bias)\n","        conv = tf.nn.relu(_x, name='relu_cnn_%d' % cnn_size)\n","        pool = tf.nn.max_pool(conv,\n","                              ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1],\n","                              strides=[1, 1, 1, 1],\n","                              padding='VALID')\n","        last_layer = pool\n","    last_layer = tf.reshape(last_layer, [batch_size, -1])\n","\n","    flat_layers_after = [49600, 3000, 800, 100, 1]\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":98,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[{"output_type":"error","ename":"TypeError","evalue":"Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=361'>362</a>\u001b[0m         \u001b[0;31m# Look up embeddings for inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=362'>363</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=363'>364</a>\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flow_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=364'>365</a>\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=365'>366</a>\u001b[0m         \u001b[0;31m# Compute the average NCE loss for the batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36mmodel_cnn\u001b[0;34m(flow_before, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=272'>273</a>\u001b[0m                               padding='VALID')\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=273'>274</a>\u001b[0m         \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=274'>275</a>\u001b[0;31m     \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=275'>276</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=276'>277</a>\u001b[0m     \u001b[0mflat_layers_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m49600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6198\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6199\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   6200\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64"]}],"metadata":{}},{"cell_type":"code","execution_count":99,"source":["def model_cnn(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","\n","    CNN_LAYERS = [[2, 20, 1, 2000, 5], [4, 10, 2000, 800, 3]]\n","\n","    for cnn_size in range(len(CNN_LAYERS)):\n","        cnn_weights = tf.get_variable(\n","            \"cnn_weight%d\" % cnn_size,\n","            CNN_LAYERS[cnn_size][:-1],\n","            initializer=tf.random_normal_initializer(stddev=0.01))\n","        cnn_bias = tf.get_variable(\"cnn_bias%d\" % cnn_size,\n","                                   [CNN_LAYERS[cnn_size][-2]],\n","                                   initializer=tf.zeros_initializer())\n","\n","        _x = tf.nn.conv2d(last_layer,\n","                          cnn_weights,\n","                          strides=[1, 2, 2, 1],\n","                          padding='VALID')\n","        _x = tf.nn.bias_add(_x, cnn_bias)\n","        conv = tf.nn.relu(_x, name='relu_cnn_%d' % cnn_size)\n","        pool = tf.nn.max_pool(conv,\n","                              ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1],\n","                              strides=[1, 1, 1, 1],\n","                              padding='VALID')\n","        last_layer = pool\n","    last_layer = tf.reshape(last_layer, [int(batch_size), -1])\n","\n","    flat_layers_after = [49600, 3000, 800, 100, 1]\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":100,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":101,"source":["num_epochs = 200\n","import datetime\n","\n","writer = tf.summary.FileWriter('./logs/tf_log/noise_classifier/allcir_300_' +\n","                               str(datetime.datetime.now()),\n","                               graph=graph)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":102,"source":["# Launch the graph\n","# with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n","# with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n","#saver = tf.train.Saver()\n","if TRAINING:\n","    with tf.Session(graph=graph) as session:\n","        # We must initialize all variables before we use them.\n","        session.run(init)\n","\n","        for epoch in xrange(num_epochs):\n","            l2s, labels, l2s_test, labels_test = generate_data(\n","                dataset=dataset,\n","                train_index=train_index,\n","                test_index=test_index,\n","                flow_size=flow_size)\n","            rr = range(len(l2s))\n","            np.random.shuffle(rr)\n","            l2s = l2s[rr]\n","            labels = labels[rr]\n","\n","            average_loss = 0\n","            new_epoch = True\n","            num_steps = (len(l2s) // batch_size) - 1\n","\n","            for step in xrange(num_steps):\n","                start_ind = step * batch_size\n","                end_ind = ((step + 1) * batch_size)\n","                if end_ind < start_ind:\n","                    print('HOOY')\n","                    continue\n","\n","                else:\n","                    batch_flow_before = l2s[start_ind:end_ind, :]\n","                    batch_label = labels[start_ind:end_ind]\n","\n","                feed_dict = {\n","                    train_flow_before: batch_flow_before,\n","                    train_label: batch_label,\n","                    dropout_keep_prob: 0.6\n","                }\n","                # We perform one update step by evaluating the optimizer op (including it\n","                # in the list of returned values for session.run()\n","\n","                _, loss_val, summary = session.run(\n","                    [optimizer, loss, summary_op], feed_dict=feed_dict)\n","\n","                # average_loss += loss_val\n","                writer.add_summary(summary, (epoch * num_steps) + step)\n","\n","                # print step, loss_val\n","                # if step % FLAGS.print_every_n_steps == 0:\n","                #     if step > 0:\n","                #         average_loss /= FLAGS.print_every_n_steps\n","                #     # The average loss is an estimate of the loss over the last 2000 batches.\n","                #     print(\"Average loss at step \", step, \": \", average_loss)\n","                #     average_loss = 0.\n","\n","                # Note that this is expensive (~20% slowdown if computed every 500 steps)\n","\n","                if ((epoch * num_steps) + step) % 100 == 0:\n","                    print(\"Average loss on validation set at step \",\n","                          (epoch * num_steps) + step, \": \", loss_val)\n","                if (((epoch * num_steps) + step)) % 3000 == 0 and epoch > 1:\n","                    tp = 0\n","                    fp = 0\n","\n","                    num_steps_test = (len(l2s_test) // batch_size) - 1\n","                    Y_est = np.zeros((batch_size * (num_steps_test + 1)))\n","                    for step in range(num_steps_test):\n","                        start_ind = step * batch_size\n","                        end_ind = ((step + 1) * batch_size)\n","                        test_batch_flow_before = l2s_test[start_ind:end_ind]\n","                        feed_dict = {\n","                            train_flow_before: test_batch_flow_before,\n","                            dropout_keep_prob: 1.0\n","                        }\n","\n","                        est = session.run(predict, feed_dict=feed_dict)\n","                        #est=np.array([xxx.sum() for xxx in test_batch_flow_before])\n","                        Y_est[start_ind:end_ind] = est.reshape((batch_size))\n","                    num_samples_test = len(l2s_test) / (negetive_samples + 1)\n","\n","                    for idx in range(num_samples_test - 1):\n","                        best = np.argmax(\n","                            Y_est[idx * (negetive_samples + 1):(idx + 1) *\n","                                  (negetive_samples + 1)])\n","\n","                        if labels_test[best + (idx *\n","                                               (negetive_samples + 1))] == 1:\n","                            tp += 1\n","                        else:\n","                            fp += 1\n","                    print(tp, fp)\n","                    acc = float(tp) / float(tp + fp)\n","                    if float(tp) / float(tp + fp) > 0.8:\n","                        print('saving...')\n","                        save_path = saver.save(\n","                            session,\n","                            \"/mnt/nfs/work1/amir/milad/tor_199_epoch%d_step%d_acc%.2f.ckpt\"\n","                            % (epoch, step, acc))\n","                        print('saved')\n","            print('Epoch', epoch)\n","            #save_path = saver.save(session, \"/mnt/nfs/scratch1/milad/model_diff_large_1e4_epoch%d.ckpt\"%(epoch))\n","\n","            #t.join()\n","else:\n","    with tf.Session(graph=graph) as session:\n","        name = input('model name')\n","        saver.restore(session, \"/mnt/nfs/work1/amir/milad/%s\" % name)\n","        print(\"Model restored.\")\n","        corrs = np.zeros((len(test_index), len(test_index)))\n","        batch = []\n","        l2s_test_all = np.zeros((batch_size, 8, flow_size, 1))\n","        l_ids = []\n","        index = 0\n","        xi, xj = 0, 0\n","        for i in tqdm.tqdm(test_index):\n","            xj = 0\n","            for j in test_index:\n","\n","                l2s_test_all[index, 0, :, 0] = np.array(\n","                    dataset[j]['here'][0]['<-'][:flow_size]) * 1000.0\n","                l2s_test_all[index, 1, :, 0] = np.array(\n","                    dataset[i]['there'][0]['->'][:flow_size]) * 1000.0\n","                l2s_test_all[index, 2, :, 0] = np.array(\n","                    dataset[i]['there'][0]['<-'][:flow_size]) * 1000.0\n","                l2s_test_all[index, 3, :, 0] = np.array(\n","                    dataset[j]['here'][0]['->'][:flow_size]) * 1000.0\n","\n","                l2s_test_all[index, 4, :, 0] = np.array(\n","                    dataset[j]['here'][1]['<-'][:flow_size]) / 1000.0\n","                l2s_test_all[index, 5, :, 0] = np.array(\n","                    dataset[i]['there'][1]['->'][:flow_size]) / 1000.0\n","                l2s_test_all[index, 6, :, 0] = np.array(\n","                    dataset[i]['there'][1]['<-'][:flow_size]) / 1000.0\n","                l2s_test_all[index, 7, :, 0] = np.array(\n","                    dataset[j]['here'][1]['->'][:flow_size]) / 1000.0\n","                l_ids.append((xi, xj))\n","                index += 1\n","                if index == batch_size:\n","                    index = 0\n","                    cor_vals = session.run(predict,\n","                                           feed_dict={\n","                                               train_flow_before: l2s_test_all,\n","                                               dropout_keep_prob: 1.0\n","                                           })\n","                    for ids in range(len(l_ids)):\n","                        di, dj = l_ids[ids]\n","                        corrs[di, dj] = cor_vals[ids]\n","                    l_ids = []\n","                xj += 1\n","            xi += 1\n","        np.save(open('correlation_values_test.np', 'w'), corrs)"],"outputs":[{"output_type":"error","ename":"NotFoundError","evalue":"/mnt/nfs/work1/amir/milad; No such file or directory","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=485'>486</a>\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=486'>487</a>\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=487'>488</a>\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/mnt/nfs/work1/amir/milad/%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=488'>489</a>\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model restored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=489'>490</a>\u001b[0m         \u001b[0mcorrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1713\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't load save_path when it is None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1715\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheckpoint_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1716\u001b[0m       raise ValueError(\"The passed save_path is not a valid checkpoint: \"\n\u001b[1;32m   1717\u001b[0m                        + compat.as_text(save_path))\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mcheckpoint_exists\u001b[0;34m(checkpoint_prefix)\u001b[0m\n\u001b[1;32m   2054\u001b[0m   pathname = _prefix_to_checkpoint_path(checkpoint_prefix,\n\u001b[1;32m   2055\u001b[0m                                         saver_pb2.SaverDef.V2)\n\u001b[0;32m-> 2056\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2057\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mget_matching_files\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    340\u001b[0m           \u001b[0;31m# Convert the filenames to string from bytes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0msingle_filename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m           for matching_filename in pywrap_tensorflow.GetMatchingFiles(\n\u001b[1;32m    344\u001b[0m               compat.as_bytes(single_filename), status)\n","\u001b[0;32m~/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotFoundError\u001b[0m: /mnt/nfs/work1/amir/milad; No such file or directory"]}],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Launch the graph\n","# with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n","# with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n","#saver = tf.train.Saver()\n","if TRAINING:\n","    with tf.Session(graph=graph) as session:\n","        # We must initialize all variables before we use them.\n","        session.run(init)\n","\n","        for epoch in xrange(num_epochs):\n","            l2s, labels, l2s_test, labels_test = generate_data(\n","                dataset=dataset,\n","                train_index=train_index,\n","                test_index=test_index,\n","                flow_size=flow_size)\n","            rr = range(len(l2s))\n","            np.random.shuffle(rr)\n","            l2s = l2s[rr]\n","            labels = labels[rr]\n","\n","            average_loss = 0\n","            new_epoch = True\n","            num_steps = (len(l2s) // batch_size) - 1\n","\n","            for step in xrange(num_steps):\n","                start_ind = step * batch_size\n","                end_ind = ((step + 1) * batch_size)\n","                if end_ind < start_ind:\n","                    print('HOOY')\n","                    continue\n","\n","                else:\n","                    batch_flow_before = l2s[start_ind:end_ind, :]\n","                    batch_label = labels[start_ind:end_ind]\n","\n","                feed_dict = {\n","                    train_flow_before: batch_flow_before,\n","                    train_label: batch_label,\n","                    dropout_keep_prob: 0.6\n","                }\n","                # We perform one update step by evaluating the optimizer op (including it\n","                # in the list of returned values for session.run()\n","\n","                _, loss_val, summary = session.run(\n","                    [optimizer, loss, summary_op], feed_dict=feed_dict)\n","\n","                # average_loss += loss_val\n","                writer.add_summary(summary, (epoch * num_steps) + step)\n","\n","                # print step, loss_val\n","                # if step % FLAGS.print_every_n_steps == 0:\n","                #     if step > 0:\n","                #         average_loss /= FLAGS.print_every_n_steps\n","                #     # The average loss is an estimate of the loss over the last 2000 batches.\n","                #     print(\"Average loss at step \", step, \": \", average_loss)\n","                #     average_loss = 0.\n","\n","                # Note that this is expensive (~20% slowdown if computed every 500 steps)\n","\n","                if ((epoch * num_steps) + step) % 100 == 0:\n","                    print(\"Average loss on validation set at step \",\n","                          (epoch * num_steps) + step, \": \", loss_val)\n","                if (((epoch * num_steps) + step)) % 3000 == 0 and epoch > 1:\n","                    tp = 0\n","                    fp = 0\n","\n","                    num_steps_test = (len(l2s_test) // batch_size) - 1\n","                    Y_est = np.zeros((batch_size * (num_steps_test + 1)))\n","                    for step in range(num_steps_test):\n","                        start_ind = step * batch_size\n","                        end_ind = ((step + 1) * batch_size)\n","                        test_batch_flow_before = l2s_test[start_ind:end_ind]\n","                        feed_dict = {\n","                            train_flow_before: test_batch_flow_before,\n","                            dropout_keep_prob: 1.0\n","                        }\n","\n","                        est = session.run(predict, feed_dict=feed_dict)\n","                        #est=np.array([xxx.sum() for xxx in test_batch_flow_before])\n","                        Y_est[start_ind:end_ind] = est.reshape((batch_size))\n","                    num_samples_test = len(l2s_test) / (negetive_samples + 1)\n","\n","                    for idx in range(num_samples_test - 1):\n","                        best = np.argmax(\n","                            Y_est[idx * (negetive_samples + 1):(idx + 1) *\n","                                  (negetive_samples + 1)])\n","\n","                        if labels_test[best + (idx *\n","                                               (negetive_samples + 1))] == 1:\n","                            tp += 1\n","                        else:\n","                            fp += 1\n","                    print(tp, fp)\n","                    acc = float(tp) / float(tp + fp)\n","                    if float(tp) / float(tp + fp) > 0.8:\n","                        print('saving...')\n","                        save_path = saver.save(\n","                            session,\n","                            \"/mnt/nfs/work1/amir/milad/tor_199_epoch%d_step%d_acc%.2f.ckpt\"\n","                            % (epoch, step, acc))\n","                        print('saved')\n","            print('Epoch', epoch)\n","            #save_path = saver.save(session, \"/mnt/nfs/scratch1/milad/model_diff_large_1e4_epoch%d.ckpt\"%(epoch))\n","\n","            #t.join()\n","else:\n","    with tf.Session(graph=graph) as session:\n","        name = input('model name')\n","        saver.restore(session, \"/work1/%s\" % name)\n","        print(\"Model restored.\")\n","        corrs = np.zeros((len(test_index), len(test_index)))\n","        batch = []\n","        l2s_test_all = np.zeros((batch_size, 8, flow_size, 1))\n","        l_ids = []\n","        index = 0\n","        xi, xj = 0, 0\n","        for i in tqdm.tqdm(test_index):\n","            xj = 0\n","            for j in test_index:\n","\n","                l2s_test_all[index, 0, :, 0] = np.array(\n","                    dataset[j]['here'][0]['<-'][:flow_size]) * 1000.0\n","                l2s_test_all[index, 1, :, 0] = np.array(\n","                    dataset[i]['there'][0]['->'][:flow_size]) * 1000.0\n","                l2s_test_all[index, 2, :, 0] = np.array(\n","                    dataset[i]['there'][0]['<-'][:flow_size]) * 1000.0\n","                l2s_test_all[index, 3, :, 0] = np.array(\n","                    dataset[j]['here'][0]['->'][:flow_size]) * 1000.0\n","\n","                l2s_test_all[index, 4, :, 0] = np.array(\n","                    dataset[j]['here'][1]['<-'][:flow_size]) / 1000.0\n","                l2s_test_all[index, 5, :, 0] = np.array(\n","                    dataset[i]['there'][1]['->'][:flow_size]) / 1000.0\n","                l2s_test_all[index, 6, :, 0] = np.array(\n","                    dataset[i]['there'][1]['<-'][:flow_size]) / 1000.0\n","                l2s_test_all[index, 7, :, 0] = np.array(\n","                    dataset[j]['here'][1]['->'][:flow_size]) / 1000.0\n","                l_ids.append((xi, xj))\n","                index += 1\n","                if index == batch_size:\n","                    index = 0\n","                    cor_vals = session.run(predict,\n","                                           feed_dict={\n","                                               train_flow_before: l2s_test_all,\n","                                               dropout_keep_prob: 1.0\n","                                           })\n","                    for ids in range(len(l_ids)):\n","                        di, dj = l_ids[ids]\n","                        corrs[di, dj] = cor_vals[ids]\n","                    l_ids = []\n","                xj += 1\n","            xi += 1\n","        np.save(open('correlation_values_test.np', 'w'), corrs)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":1,"source":["# Launch the graph\n","# with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n","# with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n","#saver = tf.train.Saver()\n","if TRAINING:\n","    with tf.Session(graph=graph) as session:\n","        # We must initialize all variables before we use them.\n","        session.run(init)\n","\n","        for epoch in xrange(num_epochs):\n","            l2s, labels, l2s_test, labels_test = generate_data(\n","                dataset=dataset,\n","                train_index=train_index,\n","                test_index=test_index,\n","                flow_size=flow_size)\n","            rr = range(len(l2s))\n","            np.random.shuffle(rr)\n","            l2s = l2s[rr]\n","            labels = labels[rr]\n","\n","            average_loss = 0\n","            new_epoch = True\n","            num_steps = (len(l2s) // batch_size) - 1\n","\n","            for step in xrange(num_steps):\n","                start_ind = step * batch_size\n","                end_ind = ((step + 1) * batch_size)\n","                if end_ind < start_ind:\n","                    print('HOOY')\n","                    continue\n","\n","                else:\n","                    batch_flow_before = l2s[start_ind:end_ind, :]\n","                    batch_label = labels[start_ind:end_ind]\n","\n","                feed_dict = {\n","                    train_flow_before: batch_flow_before,\n","                    train_label: batch_label,\n","                    dropout_keep_prob: 0.6\n","                }\n","                # We perform one update step by evaluating the optimizer op (including it\n","                # in the list of returned values for session.run()\n","\n","                _, loss_val, summary = session.run(\n","                    [optimizer, loss, summary_op], feed_dict=feed_dict)\n","\n","                # average_loss += loss_val\n","                writer.add_summary(summary, (epoch * num_steps) + step)\n","\n","                # print step, loss_val\n","                # if step % FLAGS.print_every_n_steps == 0:\n","                #     if step > 0:\n","                #         average_loss /= FLAGS.print_every_n_steps\n","                #     # The average loss is an estimate of the loss over the last 2000 batches.\n","                #     print(\"Average loss at step \", step, \": \", average_loss)\n","                #     average_loss = 0.\n","\n","                # Note that this is expensive (~20% slowdown if computed every 500 steps)\n","\n","                if ((epoch * num_steps) + step) % 100 == 0:\n","                    print(\"Average loss on validation set at step \",\n","                          (epoch * num_steps) + step, \": \", loss_val)\n","                if (((epoch * num_steps) + step)) % 3000 == 0 and epoch > 1:\n","                    tp = 0\n","                    fp = 0\n","\n","                    num_steps_test = (len(l2s_test) // batch_size) - 1\n","                    Y_est = np.zeros((batch_size * (num_steps_test + 1)))\n","                    for step in range(num_steps_test):\n","                        start_ind = step * batch_size\n","                        end_ind = ((step + 1) * batch_size)\n","                        test_batch_flow_before = l2s_test[start_ind:end_ind]\n","                        feed_dict = {\n","                            train_flow_before: test_batch_flow_before,\n","                            dropout_keep_prob: 1.0\n","                        }\n","\n","                        est = session.run(predict, feed_dict=feed_dict)\n","                        #est=np.array([xxx.sum() for xxx in test_batch_flow_before])\n","                        Y_est[start_ind:end_ind] = est.reshape((batch_size))\n","                    num_samples_test = len(l2s_test) / (negetive_samples + 1)\n","\n","                    for idx in range(num_samples_test - 1):\n","                        best = np.argmax(\n","                            Y_est[idx * (negetive_samples + 1):(idx + 1) *\n","                                  (negetive_samples + 1)])\n","\n","                        if labels_test[best + (idx *\n","                                               (negetive_samples + 1))] == 1:\n","                            tp += 1\n","                        else:\n","                            fp += 1\n","                    print(tp, fp)\n","                    acc = float(tp) / float(tp + fp)\n","                    if float(tp) / float(tp + fp) > 0.8:\n","                        print('saving...')\n","                        save_path = saver.save(\n","                            session,\n","                            \"./work1/tor_199_epoch%d_step%d_acc%.2f.ckpt\" %\n","                            (epoch, step, acc))\n","                        print('saved')\n","            print('Epoch', epoch)\n","            #save_path = saver.save(session, \"/mnt/nfs/scratch1/milad/model_diff_large_1e4_epoch%d.ckpt\"%(epoch))\n","\n","            #t.join()\n","else:\n","    with tf.Session(graph=graph) as session:\n","        name = input('model name')\n","        saver.restore(session, \"./work1/%s\" % name)\n","        print(\"Model restored.\")\n","        corrs = np.zeros((len(test_index), len(test_index)))\n","        batch = []\n","        l2s_test_all = np.zeros((batch_size, 8, flow_size, 1))\n","        l_ids = []\n","        index = 0\n","        xi, xj = 0, 0\n","        for i in tqdm.tqdm(test_index):\n","            xj = 0\n","            for j in test_index:\n","\n","                l2s_test_all[index, 0, :, 0] = np.array(\n","                    dataset[j]['here'][0]['<-'][:flow_size]) * 1000.0\n","                l2s_test_all[index, 1, :, 0] = np.array(\n","                    dataset[i]['there'][0]['->'][:flow_size]) * 1000.0\n","                l2s_test_all[index, 2, :, 0] = np.array(\n","                    dataset[i]['there'][0]['<-'][:flow_size]) * 1000.0\n","                l2s_test_all[index, 3, :, 0] = np.array(\n","                    dataset[j]['here'][0]['->'][:flow_size]) * 1000.0\n","\n","                l2s_test_all[index, 4, :, 0] = np.array(\n","                    dataset[j]['here'][1]['<-'][:flow_size]) / 1000.0\n","                l2s_test_all[index, 5, :, 0] = np.array(\n","                    dataset[i]['there'][1]['->'][:flow_size]) / 1000.0\n","                l2s_test_all[index, 6, :, 0] = np.array(\n","                    dataset[i]['there'][1]['<-'][:flow_size]) / 1000.0\n","                l2s_test_all[index, 7, :, 0] = np.array(\n","                    dataset[j]['here'][1]['->'][:flow_size]) / 1000.0\n","                l_ids.append((xi, xj))\n","                index += 1\n","                if index == batch_size:\n","                    index = 0\n","                    cor_vals = session.run(predict,\n","                                           feed_dict={\n","                                               train_flow_before: l2s_test_all,\n","                                               dropout_keep_prob: 1.0\n","                                           })\n","                    for ids in range(len(l_ids)):\n","                        di, dj = l_ids[ids]\n","                        corrs[di, dj] = cor_vals[ids]\n","                    l_ids = []\n","                xj += 1\n","            xi += 1\n","        np.save(open('correlation_values_test.np', 'w'), corrs)"],"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'TRAINING' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-84cbe7dab794>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#saver = tf.train.Saver()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mTRAINING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# We must initialize all variables before we use them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'TRAINING' is not defined"]}],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["# Launch the graph\n","# with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n","# with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n","#saver = tf.train.Saver()\n","if TRAINING:\n","    with tf.Session(graph=graph) as session:\n","        # We must initialize all variables before we use them.\n","        session.run(init)\n","\n","        for epoch in xrange(num_epochs):\n","            l2s, labels, l2s_test, labels_test = generate_data(\n","                dataset=dataset,\n","                train_index=train_index,\n","                test_index=test_index,\n","                flow_size=flow_size)\n","            rr = range(len(l2s))\n","            np.random.shuffle(rr)\n","            l2s = l2s[rr]\n","            labels = labels[rr]\n","\n","            average_loss = 0\n","            new_epoch = True\n","            num_steps = (len(l2s) // batch_size) - 1\n","\n","            for step in xrange(num_steps):\n","                start_ind = step * batch_size\n","                end_ind = ((step + 1) * batch_size)\n","                if end_ind < start_ind:\n","                    print('HOOY')\n","                    continue\n","\n","                else:\n","                    batch_flow_before = l2s[start_ind:end_ind, :]\n","                    batch_label = labels[start_ind:end_ind]\n","\n","                feed_dict = {\n","                    train_flow_before: batch_flow_before,\n","                    train_label: batch_label,\n","                    dropout_keep_prob: 0.6\n","                }\n","                # We perform one update step by evaluating the optimizer op (including it\n","                # in the list of returned values for session.run()\n","\n","                _, loss_val, summary = session.run(\n","                    [optimizer, loss, summary_op], feed_dict=feed_dict)\n","\n","                # average_loss += loss_val\n","                writer.add_summary(summary, (epoch * num_steps) + step)\n","\n","                # print step, loss_val\n","                # if step % FLAGS.print_every_n_steps == 0:\n","                #     if step > 0:\n","                #         average_loss /= FLAGS.print_every_n_steps\n","                #     # The average loss is an estimate of the loss over the last 2000 batches.\n","                #     print(\"Average loss at step \", step, \": \", average_loss)\n","                #     average_loss = 0.\n","\n","                # Note that this is expensive (~20% slowdown if computed every 500 steps)\n","\n","                if ((epoch * num_steps) + step) % 100 == 0:\n","                    print(\"Average loss on validation set at step \",\n","                          (epoch * num_steps) + step, \": \", loss_val)\n","                if (((epoch * num_steps) + step)) % 3000 == 0 and epoch > 1:\n","                    tp = 0\n","                    fp = 0\n","\n","                    num_steps_test = (len(l2s_test) // batch_size) - 1\n","                    Y_est = np.zeros((batch_size * (num_steps_test + 1)))\n","                    for step in range(num_steps_test):\n","                        start_ind = step * batch_size\n","                        end_ind = ((step + 1) * batch_size)\n","                        test_batch_flow_before = l2s_test[start_ind:end_ind]\n","                        feed_dict = {\n","                            train_flow_before: test_batch_flow_before,\n","                            dropout_keep_prob: 1.0\n","                        }\n","\n","                        est = session.run(predict, feed_dict=feed_dict)\n","                        #est=np.array([xxx.sum() for xxx in test_batch_flow_before])\n","                        Y_est[start_ind:end_ind] = est.reshape((batch_size))\n","                    num_samples_test = len(l2s_test) / (negetive_samples + 1)\n","\n","                    for idx in range(num_samples_test - 1):\n","                        best = np.argmax(\n","                            Y_est[idx * (negetive_samples + 1):(idx + 1) *\n","                                  (negetive_samples + 1)])\n","\n","                        if labels_test[best + (idx *\n","                                               (negetive_samples + 1))] == 1:\n","                            tp += 1\n","                        else:\n","                            fp += 1\n","                    print(tp, fp)\n","                    acc = float(tp) / float(tp + fp)\n","                    if float(tp) / float(tp + fp) > 0.8:\n","                        print('saving...')\n","                        save_path = saver.save(\n","                            session,\n","                            \"./work1/tor_199_epoch%d_step%d_acc%.2f.ckpt\" %\n","                            (epoch, step, acc))\n","                        print('saved')\n","            print('Epoch', epoch)\n","            #save_path = saver.save(session, \"/mnt/nfs/scratch1/milad/model_diff_large_1e4_epoch%d.ckpt\"%(epoch))\n","\n","            #t.join()\n","else:\n","    with tf.Session(graph=graph) as session:\n","        name = input('model name')\n","        saver.restore(session, \"./work1/%s\" % name)\n","        print(\"Model restored.\")\n","        corrs = np.zeros((len(test_index), len(test_index)))\n","        batch = []\n","        l2s_test_all = np.zeros((batch_size, 8, flow_size, 1))\n","        l_ids = []\n","        index = 0\n","        xi, xj = 0, 0\n","        for i in tqdm.tqdm(test_index):\n","            xj = 0\n","            for j in test_index:\n","\n","                l2s_test_all[index, 0, :, 0] = np.array(\n","                    dataset[j]['here'][0]['<-'][:flow_size]) * 1000.0\n","                l2s_test_all[index, 1, :, 0] = np.array(\n","                    dataset[i]['there'][0]['->'][:flow_size]) * 1000.0\n","                l2s_test_all[index, 2, :, 0] = np.array(\n","                    dataset[i]['there'][0]['<-'][:flow_size]) * 1000.0\n","                l2s_test_all[index, 3, :, 0] = np.array(\n","                    dataset[j]['here'][0]['->'][:flow_size]) * 1000.0\n","\n","                l2s_test_all[index, 4, :, 0] = np.array(\n","                    dataset[j]['here'][1]['<-'][:flow_size]) / 1000.0\n","                l2s_test_all[index, 5, :, 0] = np.array(\n","                    dataset[i]['there'][1]['->'][:flow_size]) / 1000.0\n","                l2s_test_all[index, 6, :, 0] = np.array(\n","                    dataset[i]['there'][1]['<-'][:flow_size]) / 1000.0\n","                l2s_test_all[index, 7, :, 0] = np.array(\n","                    dataset[j]['here'][1]['->'][:flow_size]) / 1000.0\n","                l_ids.append((xi, xj))\n","                index += 1\n","                if index == batch_size:\n","                    index = 0\n","                    cor_vals = session.run(predict,\n","                                           feed_dict={\n","                                               train_flow_before: l2s_test_all,\n","                                               dropout_keep_prob: 1.0\n","                                           })\n","                    for ids in range(len(l_ids)):\n","                        di, dj = l_ids[ids]\n","                        corrs[di, dj] = cor_vals[ids]\n","                    l_ids = []\n","                xj += 1\n","            xi += 1\n","        np.save(open('correlation_values_test.np', 'w'), corrs)"],"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'TRAINING' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=381'>382</a>\u001b[0m \u001b[0;31m# with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=382'>383</a>\u001b[0m \u001b[0;31m#saver = tf.train.Saver()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=383'>384</a>\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mTRAINING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=384'>385</a>\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=385'>386</a>\u001b[0m         \u001b[0;31m# We must initialize all variables before we use them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'TRAINING' is not defined"]}],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["import numpy as np\n","import tqdm\n","import pickle"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["flow_size = 300\n","is_training = input('train?')\n","TRAINING = True if is_training == 'y' else False"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["all_runs = {\n","    '8872': '192.168.122.117',\n","    '8802': '192.168.122.117',\n","    '8873': '192.168.122.67',\n","    '8803': '192.168.122.67',\n","    '8874': '192.168.122.113',\n","    '8804': '192.168.122.113',\n","    '8875': '192.168.122.120',\n","    '8876': '192.168.122.30',\n","    '8877': '192.168.122.208',\n","    '8878': '192.168.122.58'\n","}"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":6,"source":["# !pwd"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":7,"source":["dataset = []\n","\n","for name in all_runs:\n","    with open('./dataset/%s_tordata300.pickle' % name, 'rb') as f:\n","        dataset += pickle.load(f)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":8,"source":["if TRAINING:\n","\n","    len_tr = len(dataset)\n","    train_ratio = float(len_tr - 6000) / float(len_tr)\n","    rr = list(range(len(dataset)))\n","    np.random.shuffle(rr)\n","\n","    train_index = rr[:int(len_tr * train_ratio)]\n","    test_index = rr[int(len_tr * train_ratio):]  #range(len(dataset_test)) # #\n","    with open('test_index300.pickle', 'wb') as f:\n","        pickle.dump(test_index, f)\n","else:\n","    with open('test_index300.pickle', 'rb') as f:\n","        test_index = pickle.load(f)[:1000]"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":9,"source":["negetive_samples = 199"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":10,"source":["def generate_data(dataset, train_index, test_index, flow_size):\n","\n","    global negetive_samples\n","\n","    all_samples = len(train_index)\n","    labels = np.zeros((all_samples * (negetive_samples + 1), 1))\n","    l2s = np.zeros((all_samples * (negetive_samples + 1), 8, flow_size, 1))\n","\n","    index = 0\n","    random_ordering = [] + train_index\n","    for i in tqdm.tqdm(train_index):\n","        #[]#list(lsh.find_k_nearest_neighbors((Y_train[i]/ np.linalg.norm(Y_train[i])).astype(np.float64),(50)))\n","\n","        l2s[index, 0, :,\n","            0] = np.array(dataset[i]['here'][0]['<-'][:flow_size]) * 1000.0\n","        l2s[index, 1, :,\n","            0] = np.array(dataset[i]['there'][0]['->'][:flow_size]) * 1000.0\n","        l2s[index, 2, :,\n","            0] = np.array(dataset[i]['there'][0]['<-'][:flow_size]) * 1000.0\n","        l2s[index, 3, :,\n","            0] = np.array(dataset[i]['here'][0]['->'][:flow_size]) * 1000.0\n","\n","        l2s[index, 4, :,\n","            0] = np.array(dataset[i]['here'][1]['<-'][:flow_size]) / 1000.0\n","        l2s[index, 5, :,\n","            0] = np.array(dataset[i]['there'][1]['->'][:flow_size]) / 1000.0\n","        l2s[index, 6, :,\n","            0] = np.array(dataset[i]['there'][1]['<-'][:flow_size]) / 1000.0\n","        l2s[index, 7, :,\n","            0] = np.array(dataset[i]['here'][1]['->'][:flow_size]) / 1000.0\n","\n","        if index % (negetive_samples + 1) != 0:\n","            print(index, len(nears))\n","            raise\n","        labels[index, 0] = 1\n","        m = 0\n","        index += 1\n","        np.random.shuffle(random_ordering)\n","        for idx in random_ordering:\n","            if idx == i or m > (negetive_samples - 1):\n","                continue\n","\n","            m += 1\n","\n","            l2s[index, 0, :, 0] = np.array(\n","                dataset[idx]['here'][0]['<-'][:flow_size]) * 1000.0\n","            l2s[index, 1, :, 0] = np.array(\n","                dataset[i]['there'][0]['->'][:flow_size]) * 1000.0\n","            l2s[index, 2, :, 0] = np.array(\n","                dataset[i]['there'][0]['<-'][:flow_size]) * 1000.0\n","            l2s[index, 3, :, 0] = np.array(\n","                dataset[idx]['here'][0]['->'][:flow_size]) * 1000.0\n","\n","            l2s[index, 4, :, 0] = np.array(\n","                dataset[idx]['here'][1]['<-'][:flow_size]) / 1000.0\n","            l2s[index, 5, :, 0] = np.array(\n","                dataset[i]['there'][1]['->'][:flow_size]) / 1000.0\n","            l2s[index, 6, :, 0] = np.array(\n","                dataset[i]['there'][1]['<-'][:flow_size]) / 1000.0\n","            l2s[index, 7, :, 0] = np.array(\n","                dataset[idx]['here'][1]['->'][:flow_size]) / 1000.0\n","\n","            #l2s[index,0,:,0]=Y_train[i]#np.concatenate((Y_train[i],X_train[idx]))#(Y_train[i]*X_train[idx])/(np.linalg.norm(Y_train[i])*np.linalg.norm(X_train[idx]))\n","            #l2s[index,1,:,0]=X_train[idx]\n","\n","            labels[index, 0] = 0\n","            index += 1\n","\n","    #lsh.setup((X_test / np.linalg.norm(X_test,axis=1,keepdims=True)) .astype(np.float64))\n","    index_hard = 0\n","    num_hard_test = 0\n","    l2s_test = np.zeros(\n","        (len(test_index) * (negetive_samples + 1), 8, flow_size, 1))\n","    labels_test = np.zeros((len(test_index) * (negetive_samples + 1)))\n","    l2s_test_hard = np.zeros((num_hard_test * num_hard_test, 2, flow_size, 1))\n","    index = 0\n","    random_test = [] + test_index\n","\n","    for i in tqdm.tqdm(test_index):\n","        #list(lsh.find_k_nearest_neighbors((Y_test[i]/ np.linalg.norm(Y_test[i])).astype(np.float64),(50)))\n","\n","        if index % (negetive_samples + 1) != 0:\n","            print(index, nears)\n","            raise\n","        m = 0\n","\n","        np.random.shuffle(random_test)\n","        for idx in random_test:\n","            if idx == i or m > (negetive_samples - 1):\n","                continue\n","\n","            m += 1\n","            l2s_test[index, 0, :, 0] = np.array(\n","                dataset[idx]['here'][0]['<-'][:flow_size]) * 1000.0\n","            l2s_test[index, 1, :, 0] = np.array(\n","                dataset[i]['there'][0]['->'][:flow_size]) * 1000.0\n","            l2s_test[index, 2, :, 0] = np.array(\n","                dataset[i]['there'][0]['<-'][:flow_size]) * 1000.0\n","            l2s_test[index, 3, :, 0] = np.array(\n","                dataset[idx]['here'][0]['->'][:flow_size]) * 1000.0\n","\n","            l2s_test[index, 4, :, 0] = np.array(\n","                dataset[idx]['here'][1]['<-'][:flow_size]) / 1000.0\n","            l2s_test[index, 5, :, 0] = np.array(\n","                dataset[i]['there'][1]['->'][:flow_size]) / 1000.0\n","            l2s_test[index, 6, :, 0] = np.array(\n","                dataset[i]['there'][1]['<-'][:flow_size]) / 1000.0\n","            l2s_test[index, 7, :, 0] = np.array(\n","                dataset[idx]['here'][1]['->'][:flow_size]) / 1000.0\n","            labels_test[index] = 0\n","            index += 1\n","\n","        l2s_test[index, 0, :, 0] = np.array(\n","            dataset[i]['here'][0]['<-'][:flow_size]) * 1000.0\n","        l2s_test[index, 1, :, 0] = np.array(\n","            dataset[i]['there'][0]['->'][:flow_size]) * 1000.0\n","        l2s_test[index, 2, :, 0] = np.array(\n","            dataset[i]['there'][0]['<-'][:flow_size]) * 1000.0\n","        l2s_test[index, 3, :, 0] = np.array(\n","            dataset[i]['here'][0]['->'][:flow_size]) * 1000.0\n","\n","        l2s_test[index, 4, :, 0] = np.array(\n","            dataset[i]['here'][1]['<-'][:flow_size]) / 1000.0\n","        l2s_test[index, 5, :, 0] = np.array(\n","            dataset[i]['there'][1]['->'][:flow_size]) / 1000.0\n","        l2s_test[index, 6, :, 0] = np.array(\n","            dataset[i]['there'][1]['<-'][:flow_size]) / 1000.0\n","        l2s_test[index, 7, :, 0] = np.array(\n","            dataset[i]['here'][1]['->'][:flow_size]) / 1000.0\n","        #l2s_test[index,2,:,0]=dataset[i]['there'][0]['->'][:flow_size]\n","        #l2s_test[index,3,:,0]=dataset[i]['here'][0]['<-'][:flow_size]\n","\n","        #l2s_test[index,0,:,1]=dataset[i]['here'][1]['->'][:flow_size]\n","        #l2s_test[index,1,:,1]=dataset[i]['there'][1]['<-'][:flow_size]\n","        #l2s_test[index,2,:,1]=dataset[i]['there'][1]['->'][:flow_size]\n","        #l2s_test[index,3,:,1]=dataset[i]['here'][1]['<-'][:flow_size]\n","        labels_test[index] = 1\n","\n","        index += 1\n","    return l2s, labels, l2s_test, labels_test"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":11,"source":["import tensorflow as tf"],"outputs":[{"output_type":"stream","name":"stderr","text":["/root/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/root/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/root/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/root/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/root/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/root/anaconda3/envs/HTCoAT/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"]}],"metadata":{}},{"cell_type":"code","execution_count":12,"source":["def model(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","    flat_layers_after = [flow_size * 2, 1000, 50, 1]\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":13,"source":["def model_cnn(flow_before, dropout_keep_prob):\n","    last_layer = flow_before\n","\n","    CNN_LAYERS = [[2, 20, 1, 2000, 5], [4, 10, 2000, 800, 3]]\n","\n","    for cnn_size in range(len(CNN_LAYERS)):\n","        cnn_weights = tf.get_variable(\n","            \"cnn_weight%d\" % cnn_size,\n","            CNN_LAYERS[cnn_size][:-1],\n","            initializer=tf.random_normal_initializer(stddev=0.01))\n","        cnn_bias = tf.get_variable(\"cnn_bias%d\" % cnn_size,\n","                                   [CNN_LAYERS[cnn_size][-2]],\n","                                   initializer=tf.zeros_initializer())\n","\n","        _x = tf.nn.conv2d(last_layer,\n","                          cnn_weights,\n","                          strides=[1, 2, 2, 1],\n","                          padding='VALID')\n","        _x = tf.nn.bias_add(_x, cnn_bias)\n","        conv = tf.nn.relu(_x, name='relu_cnn_%d' % cnn_size)\n","        pool = tf.nn.max_pool(conv,\n","                              ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1],\n","                              strides=[1, 1, 1, 1],\n","                              padding='VALID')\n","        last_layer = pool\n","    last_layer = tf.reshape(last_layer, [int(batch_size), -1])\n","\n","    flat_layers_after = [49600, 3000, 800, 100, 1]\n","    for l in range(len(flat_layers_after) - 1):\n","        flat_weight = tf.get_variable(\n","            \"flat_after_weight%d\" % l,\n","            [flat_layers_after[l], flat_layers_after[l + 1]],\n","            initializer=tf.random_normal_initializer(stddev=0.01, mean=0.0))\n","\n","        flat_bias = tf.get_variable(\"flat_after_bias%d\" % l,\n","                                    [flat_layers_after[l + 1]],\n","                                    initializer=tf.zeros_initializer())\n","\n","        _x = tf.add(tf.matmul(last_layer, flat_weight), flat_bias)\n","        if l < len(flat_layers_after) - 2:\n","            _x = tf.nn.dropout(tf.nn.relu(_x, name='relu_noise_flat_%d' % l),\n","                               keep_prob=dropout_keep_prob)\n","        last_layer = _x\n","    return last_layer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":14,"source":["if TRAINING:\n","    batch_size = 256\n","    learn_rate = 0.0001\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=y2, labels=train_label),\n","                              name='loss_sigmoid')\n","\n","        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n","\n","        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","\n","        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n","\n","        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n","        s_loss = tf.summary.scalar('loss', loss)\n","        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n","        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n","        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n","\n","        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n","        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n","        #    tf.summary.scalar('gradients', gradients)\n","        summary_op = tf.summary.merge_all()\n","\n","        # Add variable initializer.\n","        init = tf.global_variables_initializer()\n","\n","        saver = tf.train.Saver()\n","\n","else:\n","    batch_size = 2804 / 2\n","\n","    graph = tf.Graph()\n","    with graph.as_default():\n","        train_flow_before = tf.placeholder(tf.float32,\n","                                           shape=[batch_size, 8, flow_size, 1],\n","                                           name='flow_before_placeholder')\n","        train_label = tf.placeholder(tf.float32,\n","                                     name='label_placeholder',\n","                                     shape=[batch_size, 1])\n","        dropout_keep_prob = tf.placeholder(tf.float32,\n","                                           name='dropout_placeholder')\n","        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n","        # Look up embeddings for inputs.\n","\n","        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n","        predict = tf.nn.sigmoid(y2)\n","        # Compute the average NCE loss for the batch.\n","        # tf.nce_loss automatically draws a new sample of the negative labels each\n","        # time we evaluate the loss.\n","        saver = tf.train.Saver()"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":15,"source":["num_epochs = 200\n","import datetime\n","\n","writer = tf.summary.FileWriter('./logs/tf_log/noise_classifier/allcir_300_' +\n","                               str(datetime.datetime.now()),\n","                               graph=graph)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":16,"source":["# Launch the graph\n","# with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n","# with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n","#saver = tf.train.Saver()\n","if TRAINING:\n","    with tf.Session(graph=graph) as session:\n","        # We must initialize all variables before we use them.\n","        session.run(init)\n","\n","        for epoch in xrange(num_epochs):\n","            l2s, labels, l2s_test, labels_test = generate_data(\n","                dataset=dataset,\n","                train_index=train_index,\n","                test_index=test_index,\n","                flow_size=flow_size)\n","            rr = range(len(l2s))\n","            np.random.shuffle(rr)\n","            l2s = l2s[rr]\n","            labels = labels[rr]\n","\n","            average_loss = 0\n","            new_epoch = True\n","            num_steps = (len(l2s) // batch_size) - 1\n","\n","            for step in xrange(num_steps):\n","                start_ind = step * batch_size\n","                end_ind = ((step + 1) * batch_size)\n","                if end_ind < start_ind:\n","                    print('HOOY')\n","                    continue\n","\n","                else:\n","                    batch_flow_before = l2s[start_ind:end_ind, :]\n","                    batch_label = labels[start_ind:end_ind]\n","\n","                feed_dict = {\n","                    train_flow_before: batch_flow_before,\n","                    train_label: batch_label,\n","                    dropout_keep_prob: 0.6\n","                }\n","                # We perform one update step by evaluating the optimizer op (including it\n","                # in the list of returned values for session.run()\n","\n","                _, loss_val, summary = session.run(\n","                    [optimizer, loss, summary_op], feed_dict=feed_dict)\n","\n","                # average_loss += loss_val\n","                writer.add_summary(summary, (epoch * num_steps) + step)\n","\n","                # print step, loss_val\n","                # if step % FLAGS.print_every_n_steps == 0:\n","                #     if step > 0:\n","                #         average_loss /= FLAGS.print_every_n_steps\n","                #     # The average loss is an estimate of the loss over the last 2000 batches.\n","                #     print(\"Average loss at step \", step, \": \", average_loss)\n","                #     average_loss = 0.\n","\n","                # Note that this is expensive (~20% slowdown if computed every 500 steps)\n","\n","                if ((epoch * num_steps) + step) % 100 == 0:\n","                    print(\"Average loss on validation set at step \",\n","                          (epoch * num_steps) + step, \": \", loss_val)\n","                if (((epoch * num_steps) + step)) % 3000 == 0 and epoch > 1:\n","                    tp = 0\n","                    fp = 0\n","\n","                    num_steps_test = (len(l2s_test) // batch_size) - 1\n","                    Y_est = np.zeros((batch_size * (num_steps_test + 1)))\n","                    for step in range(num_steps_test):\n","                        start_ind = step * batch_size\n","                        end_ind = ((step + 1) * batch_size)\n","                        test_batch_flow_before = l2s_test[start_ind:end_ind]\n","                        feed_dict = {\n","                            train_flow_before: test_batch_flow_before,\n","                            dropout_keep_prob: 1.0\n","                        }\n","\n","                        est = session.run(predict, feed_dict=feed_dict)\n","                        #est=np.array([xxx.sum() for xxx in test_batch_flow_before])\n","                        Y_est[start_ind:end_ind] = est.reshape((batch_size))\n","                    num_samples_test = len(l2s_test) / (negetive_samples + 1)\n","\n","                    for idx in range(num_samples_test - 1):\n","                        best = np.argmax(\n","                            Y_est[idx * (negetive_samples + 1):(idx + 1) *\n","                                  (negetive_samples + 1)])\n","\n","                        if labels_test[best + (idx *\n","                                               (negetive_samples + 1))] == 1:\n","                            tp += 1\n","                        else:\n","                            fp += 1\n","                    print(tp, fp)\n","                    acc = float(tp) / float(tp + fp)\n","                    if float(tp) / float(tp + fp) > 0.8:\n","                        print('saving...')\n","                        save_path = saver.save(\n","                            session,\n","                            \"./work1/tor_199_epoch%d_step%d_acc%.2f.ckpt\" %\n","                            (epoch, step, acc))\n","                        print('saved')\n","            print('Epoch', epoch)\n","            #save_path = saver.save(session, \"/mnt/nfs/scratch1/milad/model_diff_large_1e4_epoch%d.ckpt\"%(epoch))\n","\n","            #t.join()\n","else:\n","    with tf.Session(graph=graph) as session:\n","        name = input('model name')\n","        saver.restore(session, \"./work1/%s\" % name)\n","        print(\"Model restored.\")\n","        corrs = np.zeros((len(test_index), len(test_index)))\n","        batch = []\n","        l2s_test_all = np.zeros((batch_size, 8, flow_size, 1))\n","        l_ids = []\n","        index = 0\n","        xi, xj = 0, 0\n","        for i in tqdm.tqdm(test_index):\n","            xj = 0\n","            for j in test_index:\n","\n","                l2s_test_all[index, 0, :, 0] = np.array(\n","                    dataset[j]['here'][0]['<-'][:flow_size]) * 1000.0\n","                l2s_test_all[index, 1, :, 0] = np.array(\n","                    dataset[i]['there'][0]['->'][:flow_size]) * 1000.0\n","                l2s_test_all[index, 2, :, 0] = np.array(\n","                    dataset[i]['there'][0]['<-'][:flow_size]) * 1000.0\n","                l2s_test_all[index, 3, :, 0] = np.array(\n","                    dataset[j]['here'][0]['->'][:flow_size]) * 1000.0\n","\n","                l2s_test_all[index, 4, :, 0] = np.array(\n","                    dataset[j]['here'][1]['<-'][:flow_size]) / 1000.0\n","                l2s_test_all[index, 5, :, 0] = np.array(\n","                    dataset[i]['there'][1]['->'][:flow_size]) / 1000.0\n","                l2s_test_all[index, 6, :, 0] = np.array(\n","                    dataset[i]['there'][1]['<-'][:flow_size]) / 1000.0\n","                l2s_test_all[index, 7, :, 0] = np.array(\n","                    dataset[j]['here'][1]['->'][:flow_size]) / 1000.0\n","                l_ids.append((xi, xj))\n","                index += 1\n","                if index == batch_size:\n","                    index = 0\n","                    cor_vals = session.run(predict,\n","                                           feed_dict={\n","                                               train_flow_before: l2s_test_all,\n","                                               dropout_keep_prob: 1.0\n","                                           })\n","                    for ids in range(len(l_ids)):\n","                        di, dj = l_ids[ids]\n","                        corrs[di, dj] = cor_vals[ids]\n","                    l_ids = []\n","                xj += 1\n","            xi += 1\n","        np.save(open('correlation_values_test.np', 'w'), corrs)"],"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'xrange' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=386'>387</a>\u001b[0m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=387'>388</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=388'>389</a>\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=389'>390</a>\u001b[0m             l2s, labels, l2s_test, labels_test = generate_data(\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=390'>391</a>\u001b[0m                 \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'xrange' is not defined"]}],"metadata":{}},{"cell_type":"code","execution_count":17,"source":["# Launch the graph\n","# with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n","# with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n","#saver = tf.train.Saver()\n","if TRAINING:\n","    with tf.Session(graph=graph) as session:\n","        # We must initialize all variables before we use them.\n","        session.run(init)\n","\n","        for epoch in range(num_epochs):\n","            l2s, labels, l2s_test, labels_test = generate_data(\n","                dataset=dataset,\n","                train_index=train_index,\n","                test_index=test_index,\n","                flow_size=flow_size)\n","            rr = range(len(l2s))\n","            np.random.shuffle(rr)\n","            l2s = l2s[rr]\n","            labels = labels[rr]\n","\n","            average_loss = 0\n","            new_epoch = True\n","            num_steps = (len(l2s) // batch_size) - 1\n","\n","            for step in range(num_steps):\n","                start_ind = step * batch_size\n","                end_ind = ((step + 1) * batch_size)\n","                if end_ind < start_ind:\n","                    print('HOOY')\n","                    continue\n","\n","                else:\n","                    batch_flow_before = l2s[start_ind:end_ind, :]\n","                    batch_label = labels[start_ind:end_ind]\n","\n","                feed_dict = {\n","                    train_flow_before: batch_flow_before,\n","                    train_label: batch_label,\n","                    dropout_keep_prob: 0.6\n","                }\n","                # We perform one update step by evaluating the optimizer op (including it\n","                # in the list of returned values for session.run()\n","\n","                _, loss_val, summary = session.run(\n","                    [optimizer, loss, summary_op], feed_dict=feed_dict)\n","\n","                # average_loss += loss_val\n","                writer.add_summary(summary, (epoch * num_steps) + step)\n","\n","                # print step, loss_val\n","                # if step % FLAGS.print_every_n_steps == 0:\n","                #     if step > 0:\n","                #         average_loss /= FLAGS.print_every_n_steps\n","                #     # The average loss is an estimate of the loss over the last 2000 batches.\n","                #     print(\"Average loss at step \", step, \": \", average_loss)\n","                #     average_loss = 0.\n","\n","                # Note that this is expensive (~20% slowdown if computed every 500 steps)\n","\n","                if ((epoch * num_steps) + step) % 100 == 0:\n","                    print(\"Average loss on validation set at step \",\n","                          (epoch * num_steps) + step, \": \", loss_val)\n","                if (((epoch * num_steps) + step)) % 3000 == 0 and epoch > 1:\n","                    tp = 0\n","                    fp = 0\n","\n","                    num_steps_test = (len(l2s_test) // batch_size) - 1\n","                    Y_est = np.zeros((batch_size * (num_steps_test + 1)))\n","                    for step in range(num_steps_test):\n","                        start_ind = step * batch_size\n","                        end_ind = ((step + 1) * batch_size)\n","                        test_batch_flow_before = l2s_test[start_ind:end_ind]\n","                        feed_dict = {\n","                            train_flow_before: test_batch_flow_before,\n","                            dropout_keep_prob: 1.0\n","                        }\n","\n","                        est = session.run(predict, feed_dict=feed_dict)\n","                        #est=np.array([xxx.sum() for xxx in test_batch_flow_before])\n","                        Y_est[start_ind:end_ind] = est.reshape((batch_size))\n","                    num_samples_test = len(l2s_test) / (negetive_samples + 1)\n","\n","                    for idx in range(num_samples_test - 1):\n","                        best = np.argmax(\n","                            Y_est[idx * (negetive_samples + 1):(idx + 1) *\n","                                  (negetive_samples + 1)])\n","\n","                        if labels_test[best + (idx *\n","                                               (negetive_samples + 1))] == 1:\n","                            tp += 1\n","                        else:\n","                            fp += 1\n","                    print(tp, fp)\n","                    acc = float(tp) / float(tp + fp)\n","                    if float(tp) / float(tp + fp) > 0.8:\n","                        print('saving...')\n","                        save_path = saver.save(\n","                            session,\n","                            \"./work1/tor_199_epoch%d_step%d_acc%.2f.ckpt\" %\n","                            (epoch, step, acc))\n","                        print('saved')\n","            print('Epoch', epoch)\n","            #save_path = saver.save(session, \"/mnt/nfs/scratch1/milad/model_diff_large_1e4_epoch%d.ckpt\"%(epoch))\n","\n","            #t.join()\n","else:\n","    with tf.Session(graph=graph) as session:\n","        name = input('model name')\n","        saver.restore(session, \"./work1/%s\" % name)\n","        print(\"Model restored.\")\n","        corrs = np.zeros((len(test_index), len(test_index)))\n","        batch = []\n","        l2s_test_all = np.zeros((batch_size, 8, flow_size, 1))\n","        l_ids = []\n","        index = 0\n","        xi, xj = 0, 0\n","        for i in tqdm.tqdm(test_index):\n","            xj = 0\n","            for j in test_index:\n","\n","                l2s_test_all[index, 0, :, 0] = np.array(\n","                    dataset[j]['here'][0]['<-'][:flow_size]) * 1000.0\n","                l2s_test_all[index, 1, :, 0] = np.array(\n","                    dataset[i]['there'][0]['->'][:flow_size]) * 1000.0\n","                l2s_test_all[index, 2, :, 0] = np.array(\n","                    dataset[i]['there'][0]['<-'][:flow_size]) * 1000.0\n","                l2s_test_all[index, 3, :, 0] = np.array(\n","                    dataset[j]['here'][0]['->'][:flow_size]) * 1000.0\n","\n","                l2s_test_all[index, 4, :, 0] = np.array(\n","                    dataset[j]['here'][1]['<-'][:flow_size]) / 1000.0\n","                l2s_test_all[index, 5, :, 0] = np.array(\n","                    dataset[i]['there'][1]['->'][:flow_size]) / 1000.0\n","                l2s_test_all[index, 6, :, 0] = np.array(\n","                    dataset[i]['there'][1]['<-'][:flow_size]) / 1000.0\n","                l2s_test_all[index, 7, :, 0] = np.array(\n","                    dataset[j]['here'][1]['->'][:flow_size]) / 1000.0\n","                l_ids.append((xi, xj))\n","                index += 1\n","                if index == batch_size:\n","                    index = 0\n","                    cor_vals = session.run(predict,\n","                                           feed_dict={\n","                                               train_flow_before: l2s_test_all,\n","                                               dropout_keep_prob: 1.0\n","                                           })\n","                    for ids in range(len(l_ids)):\n","                        di, dj = l_ids[ids]\n","                        corrs[di, dj] = cor_vals[ids]\n","                    l_ids = []\n","                xj += 1\n","            xi += 1\n","        np.save(open('correlation_values_test.np', 'w'), corrs)"],"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1324/1324 [01:22<00:00, 15.99it/s]\n","100%|██████████| 6000/6000 [06:18<00:00, 15.84it/s]\n"]},{"output_type":"error","ename":"TypeError","evalue":"'range' object does not support item assignment","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/data/hierarchical-tc-at/DeepCorr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=393'>394</a>\u001b[0m                 flow_size=flow_size)\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=394'>395</a>\u001b[0m             \u001b[0mrr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=395'>396</a>\u001b[0;31m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=396'>397</a>\u001b[0m             \u001b[0ml2s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     <a href='file:///data/hierarchical-tc-at/DeepCorr.py?line=397'>398</a>\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.shuffle\u001b[0;34m()\u001b[0m\n","\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.shuffle\u001b[0;34m()\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'range' object does not support item assignment"]}],"metadata":{}},{"cell_type":"code","execution_count":18,"source":["# Launch the graph\n","# with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n","# with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n","#saver = tf.train.Saver()\n","if TRAINING:\n","    with tf.Session(graph=graph) as session:\n","        # We must initialize all variables before we use them.\n","        session.run(init)\n","\n","        for epoch in range(num_epochs):\n","            l2s, labels, l2s_test, labels_test = generate_data(\n","                dataset=dataset,\n","                train_index=train_index,\n","                test_index=test_index,\n","                flow_size=flow_size)\n","            rr = range(len(l2s))\n","            np.random.shuffle(list(rr))\n","            l2s = l2s[rr]\n","            labels = labels[rr]\n","\n","            average_loss = 0\n","            new_epoch = True\n","            num_steps = (len(l2s) // batch_size) - 1\n","\n","            for step in range(num_steps):\n","                start_ind = step * batch_size\n","                end_ind = ((step + 1) * batch_size)\n","                if end_ind < start_ind:\n","                    print('HOOY')\n","                    continue\n","\n","                else:\n","                    batch_flow_before = l2s[start_ind:end_ind, :]\n","                    batch_label = labels[start_ind:end_ind]\n","\n","                feed_dict = {\n","                    train_flow_before: batch_flow_before,\n","                    train_label: batch_label,\n","                    dropout_keep_prob: 0.6\n","                }\n","                # We perform one update step by evaluating the optimizer op (including it\n","                # in the list of returned values for session.run()\n","\n","                _, loss_val, summary = session.run(\n","                    [optimizer, loss, summary_op], feed_dict=feed_dict)\n","\n","                # average_loss += loss_val\n","                writer.add_summary(summary, (epoch * num_steps) + step)\n","\n","                # print step, loss_val\n","                # if step % FLAGS.print_every_n_steps == 0:\n","                #     if step > 0:\n","                #         average_loss /= FLAGS.print_every_n_steps\n","                #     # The average loss is an estimate of the loss over the last 2000 batches.\n","                #     print(\"Average loss at step \", step, \": \", average_loss)\n","                #     average_loss = 0.\n","\n","                # Note that this is expensive (~20% slowdown if computed every 500 steps)\n","\n","                if ((epoch * num_steps) + step) % 100 == 0:\n","                    print(\"Average loss on validation set at step \",\n","                          (epoch * num_steps) + step, \": \", loss_val)\n","                if (((epoch * num_steps) + step)) % 3000 == 0 and epoch > 1:\n","                    tp = 0\n","                    fp = 0\n","\n","                    num_steps_test = (len(l2s_test) // batch_size) - 1\n","                    Y_est = np.zeros((batch_size * (num_steps_test + 1)))\n","                    for step in range(num_steps_test):\n","                        start_ind = step * batch_size\n","                        end_ind = ((step + 1) * batch_size)\n","                        test_batch_flow_before = l2s_test[start_ind:end_ind]\n","                        feed_dict = {\n","                            train_flow_before: test_batch_flow_before,\n","                            dropout_keep_prob: 1.0\n","                        }\n","\n","                        est = session.run(predict, feed_dict=feed_dict)\n","                        #est=np.array([xxx.sum() for xxx in test_batch_flow_before])\n","                        Y_est[start_ind:end_ind] = est.reshape((batch_size))\n","                    num_samples_test = len(l2s_test) / (negetive_samples + 1)\n","\n","                    for idx in range(num_samples_test - 1):\n","                        best = np.argmax(\n","                            Y_est[idx * (negetive_samples + 1):(idx + 1) *\n","                                  (negetive_samples + 1)])\n","\n","                        if labels_test[best + (idx *\n","                                               (negetive_samples + 1))] == 1:\n","                            tp += 1\n","                        else:\n","                            fp += 1\n","                    print(tp, fp)\n","                    acc = float(tp) / float(tp + fp)\n","                    if float(tp) / float(tp + fp) > 0.8:\n","                        print('saving...')\n","                        save_path = saver.save(\n","                            session,\n","                            \"./work1/tor_199_epoch%d_step%d_acc%.2f.ckpt\" %\n","                            (epoch, step, acc))\n","                        print('saved')\n","            print('Epoch', epoch)\n","            #save_path = saver.save(session, \"/mnt/nfs/scratch1/milad/model_diff_large_1e4_epoch%d.ckpt\"%(epoch))\n","\n","            #t.join()\n","else:\n","    with tf.Session(graph=graph) as session:\n","        name = input('model name')\n","        saver.restore(session, \"./work1/%s\" % name)\n","        print(\"Model restored.\")\n","        corrs = np.zeros((len(test_index), len(test_index)))\n","        batch = []\n","        l2s_test_all = np.zeros((batch_size, 8, flow_size, 1))\n","        l_ids = []\n","        index = 0\n","        xi, xj = 0, 0\n","        for i in tqdm.tqdm(test_index):\n","            xj = 0\n","            for j in test_index:\n","\n","                l2s_test_all[index, 0, :, 0] = np.array(\n","                    dataset[j]['here'][0]['<-'][:flow_size]) * 1000.0\n","                l2s_test_all[index, 1, :, 0] = np.array(\n","                    dataset[i]['there'][0]['->'][:flow_size]) * 1000.0\n","                l2s_test_all[index, 2, :, 0] = np.array(\n","                    dataset[i]['there'][0]['<-'][:flow_size]) * 1000.0\n","                l2s_test_all[index, 3, :, 0] = np.array(\n","                    dataset[j]['here'][0]['->'][:flow_size]) * 1000.0\n","\n","                l2s_test_all[index, 4, :, 0] = np.array(\n","                    dataset[j]['here'][1]['<-'][:flow_size]) / 1000.0\n","                l2s_test_all[index, 5, :, 0] = np.array(\n","                    dataset[i]['there'][1]['->'][:flow_size]) / 1000.0\n","                l2s_test_all[index, 6, :, 0] = np.array(\n","                    dataset[i]['there'][1]['<-'][:flow_size]) / 1000.0\n","                l2s_test_all[index, 7, :, 0] = np.array(\n","                    dataset[j]['here'][1]['->'][:flow_size]) / 1000.0\n","                l_ids.append((xi, xj))\n","                index += 1\n","                if index == batch_size:\n","                    index = 0\n","                    cor_vals = session.run(predict,\n","                                           feed_dict={\n","                                               train_flow_before: l2s_test_all,\n","                                               dropout_keep_prob: 1.0\n","                                           })\n","                    for ids in range(len(l_ids)):\n","                        di, dj = l_ids[ids]\n","                        corrs[di, dj] = cor_vals[ids]\n","                    l_ids = []\n","                xj += 1\n","            xi += 1\n","        np.save(open('correlation_values_test.np', 'w'), corrs)"],"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1324/1324 [01:22<00:00, 16.10it/s]\n","100%|██████████| 6000/6000 [06:18<00:00, 15.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Average loss on validation set at step  0 :  0.60007966\n","Average loss on validation set at step  100 :  0.060442373\n","Average loss on validation set at step  200 :  0.05340763\n","Average loss on validation set at step  300 :  0.068456054\n","Average loss on validation set at step  400 :  0.03652566\n","Average loss on validation set at step  500 :  0.040435158\n","Average loss on validation set at step  600 :  0.05678615\n","Average loss on validation set at step  700 :  0.06839917\n","Average loss on validation set at step  800 :  0.053628955\n","Average loss on validation set at step  900 :  0.029129568\n","Average loss on validation set at step  1000 :  0.047107216\n","Epoch 0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1324/1324 [01:22<00:00, 16.10it/s]\n","100%|██████████| 6000/6000 [06:21<00:00, 15.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Average loss on validation set at step  1100 :  0.040186595\n","Average loss on validation set at step  1200 :  0.036545683\n","Average loss on validation set at step  1300 :  0.04459725\n","Average loss on validation set at step  1400 :  0.030175027\n","Average loss on validation set at step  1500 :  0.022734134\n","Average loss on validation set at step  1600 :  0.013274748\n","Average loss on validation set at step  1700 :  0.028674776\n","Average loss on validation set at step  1800 :  0.03348022\n","Average loss on validation set at step  1900 :  0.033553045\n"]}],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}